{"cells":[{"cell_type":"markdown","metadata":{"id":"hpJDBsDpst6v"},"source":["# 1. Loading prereqs"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":342,"status":"ok","timestamp":1719501894657,"user":{"displayName":"Kekko Iuorio","userId":"10763977159809291578"},"user_tz":-120},"id":"LqlTb21Ktlit","outputId":"02327c87-7f8b-4f1a-a154-9949f230d711"},"outputs":[{"name":"stdout","output_type":"stream","text":["Not running on Google Colab. \n"]}],"source":["try:\n","    from google.colab import drive\n","    IN_COLAB = True\n","    print(\"Running on Google Colab. \")\n","except:\n","    IN_COLAB = False\n","    print(\"Not running on Google Colab. \")"]},{"cell_type":"code","execution_count":56,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55752,"status":"ok","timestamp":1719501991561,"user":{"displayName":"Kekko Iuorio","userId":"10763977159809291578"},"user_tz":-120},"id":"Wj6g0-9XOEv-","outputId":"21d6c00a-aabb-4c9a-ef23-306c0361b9d0"},"outputs":[],"source":["if IN_COLAB:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    import os\n","    os.chdir(\"/content/drive/Shareddrives/AI4CYBSEC\")"]},{"cell_type":"code","execution_count":61,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"elapsed":340,"status":"ok","timestamp":1719502001041,"user":{"displayName":"Kekko Iuorio","userId":"10763977159809291578"},"user_tz":-120},"id":"IQl4L6wfOHx2","outputId":"7bd12b07-dbe8-4c4c-b5b0-838a070957ef"},"outputs":[{"data":{"text/plain":["'g:\\\\Drive condivisi\\\\AI4CYBSEC'"]},"execution_count":61,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","os.getcwd()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1k0HDZVtNfqL"},"outputs":[],"source":["NN1_WITH_DEFENCE=True\n","NN1_FLAG=False\n","NN2_FLAG=False"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22277,"status":"ok","timestamp":1719501919423,"user":{"displayName":"Kekko Iuorio","userId":"10763977159809291578"},"user_tz":-120},"id":"oZGh_8zcsCOL","outputId":"f64cf1fa-9430-44a5-a388-8647e18a5382"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: facenet-pytorch in /usr/local/lib/python3.10/dist-packages (2.6.0)\n","Requirement already satisfied: numpy<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (1.25.2)\n","Requirement already satisfied: Pillow<10.3.0,>=10.2.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (10.2.0)\n","Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (2.31.0)\n","Requirement already satisfied: torch<2.3.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (2.2.2)\n","Requirement already satisfied: torchvision<0.18.0,>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (0.17.2)\n","Requirement already satisfied: tqdm<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (4.66.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2024.6.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.15.3)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<2.3.0,>=2.2.0->facenet-pytorch) (12.5.40)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.3.0,>=2.2.0->facenet-pytorch) (2.1.5)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.3.0,>=2.2.0->facenet-pytorch) (1.3.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (10.2.0)\n"]}],"source":["if IN_COLAB:\n","    !pip install facenet-pytorch  # fornisce modelli pre-addestrati PyTorch per compiti di riconoscimento facciale\n","    !pip install Pillow # aggiunge il supporto per l'apertura, la manipolazione e il salvataggio di molti diversi formati di file immagine.\n"]},{"cell_type":"markdown","metadata":{"id":"oGkN-QeztIe5"},"source":["### 1.0 Load Model NN1"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86,"referenced_widgets":["f5e29fe5fb32474997b9b51da58747c1","deeffe98ebcb43259bc3a413a70fd114","b668e3fa0c5c4bc5b6a5d8a7252cbb95","a6b3bc121c4f4a4499146e843fc0450b","65ecd141d8084cfbb3e82e6527b156d2","18d5e290aec14797878c9fa7204db755","7735c539218e46d99592c99166f64b2a","770ea654644745bb93ae93e1783de921","4cb6d5cdc91d4883b5fb4901599fca69","a770f3c95cb24128a38346aa8e2e633b","496859beee7a4269a660f6825ec0a005"]},"executionInfo":{"elapsed":7032,"status":"ok","timestamp":1719502014002,"user":{"displayName":"Kekko Iuorio","userId":"10763977159809291578"},"user_tz":-120},"id":"amn7r7wEtWcm","outputId":"c2851362-12ef-4652-c1e4-f033e90be70d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running on device: cuda:0\n"]}],"source":["# utilizzo la libreria facenet_pytorch per caricare il modello InceptionResnetV1 preaddestrato sul dataset VGGFace2 e abilitare la classificazione.\n","from facenet_pytorch import InceptionResnetV1, MTCNN\n","import torch\n","\n","resnet = InceptionResnetV1(pretrained='vggface2').eval()\n","resnet.classify = True\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print('Running on device: {}'.format(device))\n","resnet = resnet.to(device)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8576,"status":"ok","timestamp":1719502026695,"user":{"displayName":"Kekko Iuorio","userId":"10763977159809291578"},"user_tz":-120},"id":"DnXoalzuvocQ","outputId":"25ef0444-654c-446a-8a82-8625a2e26225"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","\n","# Il modello è addestrato sulle seguenti Labels:\n","# Carico le labels del dataset VGGFACE\n","fpath = tf.keras.utils.get_file('rcmalli_vggface_labels_v2.npy',\n","                             \"https://github.com/rcmalli/keras-vggface/releases/download/v2.0/rcmalli_vggface_labels_v2.npy\",\n","                             cache_subdir=\"./\")\n","LABELS = np.load(fpath) # List of name\n","# Clean list of name\n","for i in range(len(LABELS)):\n","  LABELS[i] = LABELS[i].strip().replace(' ', '').replace('\"', '')\n"]},{"cell_type":"markdown","metadata":{"id":"nM3icdqvtc68"},"source":["#### 1.1 Load Test Set"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Ui_Lcf5rtt63"},"outputs":[],"source":["# set the path for the dataset\n","\n","if IN_COLAB:\n","  path_dataset = \"/content/drive/Shareddrives/AI4CYBSEC/face_dataset\"\n","else:\n","  path_dataset = \"G:\\Drive condivisi\\AI4CYBSEC\\\\face_dataset\"\n","identity_meta_NN1_name = \"meta_identity_NN1.csv\"\n","\n","import pandas as pd\n","import os\n","\n","path_identity_csv =os.path.join(path_dataset,identity_meta_NN1_name)\n","identity_meta_NN1 = pd.read_csv(path_identity_csv)"]},{"cell_type":"markdown","metadata":{"id":"Omx0DSUNPKht"},"source":["#### 1.2 Mapping label for NN1"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"r6JmqZ9Jt7Fy"},"outputs":[],"source":["# I want a dictonary related to the label of the Test Set that map the name of celebrities with label associated\n","name_to_id = {}\n","id_to_name = {}\n","for index, row in identity_meta_NN1.iterrows():\n","    # Ora puoi accedere ai valori di ogni riga come segue:\n","    class_id = row['Class_ID']\n","    name = row['Name']\n","    name_to_id[name]=class_id\n","    id_to_name[class_id] = name"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"a0QeyECuz1Uc"},"outputs":[],"source":["from PIL import Image\n","import os\n","import torch\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader\n","\n","class VGGFace2Dataset(Dataset):\n","    def __init__(self, root_dir, image_size=(160, 160), transform=None):\n","        self.root_dir = root_dir\n","        self.image_size = image_size\n","        self.transform = transform\n","\n","        # List of files in the dataset\n","        self.file_list = []\n","        for root, dirs, files in os.walk(self.root_dir):\n","            for file in files:\n","                self.file_list.append(os.path.join(root, file))\n","\n","    def __len__(self):\n","        return len(self.file_list)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.file_list[idx]\n","        img = Image.open(img_path).resize(self.image_size)\n","\n","        # Extract the label from the file path\n","        label = os.path.split(os.path.dirname(img_path))[-1]\n","\n","        if self.transform:\n","            img = self.transform(img)\n","\n","        return img, label"]},{"cell_type":"markdown","metadata":{"id":"YOHuDfg_PdQD"},"source":["### 2.0 Definizione dataset NN1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8915,"status":"ok","timestamp":1719502051939,"user":{"displayName":"Kekko Iuorio","userId":"10763977159809291578"},"user_tz":-120},"id":"pHCkGwmUz55L","outputId":"cd8106bb-dd1c-4d74-b856-3c799bf9bd6d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset length: 998\n"]}],"source":["# Create transform for image resizing and normalization\n","data_transform = transforms.Compose([\n","    transforms.Resize((160, 160)),\n","    transforms.ToTensor()\n","])\n","if IN_COLAB:\n","  test_set_path = \"/content/drive/Shareddrives/AI4CYBSEC/face_dataset/test_set_MTCNN\"\n","else:\n","  test_set_path = \"./face_dataset/test_set_MTCNN\"\n","# Define dataset\n","dataset = VGGFace2Dataset(root_dir=test_set_path, transform=data_transform)\n","dataset_len = len(dataset)\n","# Check the length of the dataset\n","print(\"Dataset length:\", dataset_len)\n","# Create DataLoader\n","batch_size = 1\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"ZuoAR-auuGBF"},"source":["#### 2.1 Utility Function for NN1 with mapping labels"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"Vyi1fNqYuJ3_"},"outputs":[],"source":["\n","from PIL import Image\n","from torchvision import transforms\n","import torch\n","import numpy as np\n","from matplotlib import pyplot as plt\n","from tqdm import tqdm\n","\n","def load_image(file_path):\n","    \"\"\" carica un'immagine da un percorso e la apre come un'immagine utilizzando Image.open dal modulo Pillow.\n","    Successivamente, ridimensiona l'immagine a dimensioni 160x160 pixel e la converte in un tensore utilizzando\n","    transforms.ToTensor() dal modulo torchvision.transforms.\n","    Infine, restituisce sia il tensore dell'immagine che l'immagine aperta.\n","    \"\"\"\n","    rsz = Image.open(file_path).resize((160, 160))\n","    tns = transforms.ToTensor()(rsz)\n","    return tns, rsz\n","def make_inference(model, image_tensors, name_to_id, device):\n","    \"\"\"\n","    Takes input image tensor and returns the label associated with the network's prediction.\n","\n","    \"\"\"\n","    # Move image tensors to the specified device\n","    image_tensors = image_tensors.to(device)\n","\n","    probs = model(image_tensors)\n","    #print(\"probs\", probs)\n","\n","    # Get the number of elements along the first dimension\n","    num_elements = probs.size(0)\n","\n","    # Initialize two lists to store the argmax\n","    argmax_list_1 = []\n","    argmax_list_2 = []\n","\n","    # Compute argmax for each element along the first dimension\n","    for i in range(num_elements):\n","        target_class = np.array(probs[i].detach().cpu().numpy()).argmax()  # Move to CPU for numpy operations\n","        argmax_list_1.append(name_to_id[LABELS[target_class]])\n","        argmax_list_2.append(target_class)\n","\n","    return argmax_list_1, argmax_list_2\n","\n","def validate(dataset, model, name_to_id, device):\n","    \"\"\"\n","    Validates a model on a dataset and returns the accuracy.\n","\n","    Args:\n","        dataset: Dataloader to validate the model on.\n","        model: Model to validate.\n","        device: Device to perform inference on.\n","\n","    Returns:\n","        accuracy: Accuracy of the model on the dataset.\n","    \"\"\"\n","    model.eval()\n","    correct_predictions = 0\n","    total_samples = len(dataset) * dataset.batch_size\n","\n","    with torch.no_grad():  # Disable gradient calculation\n","        for images, labels in tqdm(dataset, desc=\"Validating model\"):\n","            #images = mtcnn(images)\n","            predicted_classes, _ = make_inference(model, images, name_to_id, device)\n","            correct_predictions += sum(pred == label for pred, label in zip(predicted_classes, labels))\n","\n","    # Compute accuracy\n","    accuracy = correct_predictions / total_samples\n","    return accuracy\n","\n","\n","def plot_image(original_image, original_label):\n","  \"\"\"\n","  prende in ingresso le PIL.Image del campione originale e del corrispondete adversarial sample e li plotta\n","  \"\"\"\n","  plt.figure()\n","  plt.matshow(original_image)\n","  plt.title(\"Model Prediction: {}\".format(original_label))\n","  plt.show()"]},{"cell_type":"markdown","metadata":{"id":"b0O-EkBP4AAR"},"source":["### 3.0 Validation on Clean Data on NN1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1446988,"status":"ok","timestamp":1719493784124,"user":{"displayName":"SARA MASSARO","userId":"11677696806969070282"},"user_tz":-120},"id":"1tBQZDbJ4GvB","outputId":"4a94eb4a-79d2-4d5a-9b3e-a36e7f167a83"},"outputs":[{"name":"stderr","output_type":"stream","text":["Validating model: 100%|██████████| 998/998 [24:06<00:00,  1.45s/it]"]},{"name":"stdout","output_type":"stream","text":["\n"," Accuracy without defense 0.8316633266533067\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["acc = validate(dataloader, resnet, name_to_id, device)\n","print(\"\\n Accuracy without defense \"+str(acc))"]},{"cell_type":"markdown","metadata":{"id":"623mhk1cwIdl"},"source":["### 4.0 Implementing Deepfool Error Generic attack in ART"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19152,"status":"ok","timestamp":1719502082471,"user":{"displayName":"Kekko Iuorio","userId":"10763977159809291578"},"user_tz":-120},"id":"UZT5DCHqwo2P","outputId":"906ec381-435c-4d24-830a-1fce6c0399d0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting adversarial-robustness-toolbox[all]\n","  Downloading adversarial_robustness_toolbox-1.18.0-py3-none-any.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox[all]) (1.25.2)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox[all]) (1.11.4)\n","Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox[all]) (1.2.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox[all]) (1.16.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox[all]) (67.7.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox[all]) (4.66.4)\n","Collecting mxnet (from adversarial-robustness-toolbox[all])\n","  Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl (49.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting catboost (from adversarial-robustness-toolbox[all])\n","  Downloading catboost-1.2.5-cp310-cp310-manylinux2014_x86_64.whl (98.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox[all]) (4.1.0)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox[all]) (2.15.0)\n","Collecting tensorflow-addons (from adversarial-robustness-toolbox[all])\n","  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox[all]) (3.9.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox[all]) (2.2.2)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox[all]) (0.17.2)\n","Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox[all]) (2.0.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox[all]) (2.0.3)\n","Collecting kornia (from adversarial-robustness-toolbox[all])\n","  Downloading kornia-0.7.2-py2.py3-none-any.whl (825 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.4/825.4 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox[all]) (3.7.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox[all]) (10.2.0)\n","Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox[all]) (0.14.2)\n","Collecting pydub (from adversarial-robustness-toolbox[all])\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Collecting resampy (from adversarial-robustness-toolbox[all])\n","  Downloading resampy-0.4.3-py3-none-any.whl (3.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ffmpeg-python (from adversarial-robustness-toolbox[all])\n","  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n","Collecting cma (from adversarial-robustness-toolbox[all])\n","  Downloading cma-3.3.0-py3-none-any.whl (260 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.7/260.7 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox[all]) (0.10.2.post1)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox[all]) (4.8.0.76)\n","Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox[all]) (0.58.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->adversarial-robustness-toolbox[all]) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->adversarial-robustness-toolbox[all]) (3.5.0)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost->adversarial-robustness-toolbox[all]) (0.20.3)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost->adversarial-robustness-toolbox[all]) (5.15.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->adversarial-robustness-toolbox[all]) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->adversarial-robustness-toolbox[all]) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->adversarial-robustness-toolbox[all]) (2024.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python->adversarial-robustness-toolbox[all]) (0.18.3)\n","Collecting kornia-rs>=0.1.0 (from kornia->adversarial-robustness-toolbox[all])\n","  Downloading kornia_rs-0.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kornia->adversarial-robustness-toolbox[all]) (24.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->adversarial-robustness-toolbox[all]) (3.15.3)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->adversarial-robustness-toolbox[all]) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->adversarial-robustness-toolbox[all]) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->adversarial-robustness-toolbox[all]) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->adversarial-robustness-toolbox[all]) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->adversarial-robustness-toolbox[all]) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->adversarial-robustness-toolbox[all]) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->adversarial-robustness-toolbox[all]) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->adversarial-robustness-toolbox[all]) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->adversarial-robustness-toolbox[all]) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->adversarial-robustness-toolbox[all]) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->adversarial-robustness-toolbox[all]) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->adversarial-robustness-toolbox[all]) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->adversarial-robustness-toolbox[all]) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->adversarial-robustness-toolbox[all]) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->adversarial-robustness-toolbox[all]) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->adversarial-robustness-toolbox[all]) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->adversarial-robustness-toolbox[all]) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->adversarial-robustness-toolbox[all]) (12.5.40)\n","Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->adversarial-robustness-toolbox[all]) (3.0.1)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa->adversarial-robustness-toolbox[all]) (4.4.2)\n","Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa->adversarial-robustness-toolbox[all]) (0.12.1)\n","Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa->adversarial-robustness-toolbox[all]) (1.8.2)\n","Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa->adversarial-robustness-toolbox[all]) (0.3.7)\n","Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa->adversarial-robustness-toolbox[all]) (0.4)\n","Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->adversarial-robustness-toolbox[all]) (1.0.8)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->adversarial-robustness-toolbox[all]) (0.41.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->adversarial-robustness-toolbox[all]) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->adversarial-robustness-toolbox[all]) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->adversarial-robustness-toolbox[all]) (4.53.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->adversarial-robustness-toolbox[all]) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->adversarial-robustness-toolbox[all]) (3.1.2)\n","Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet->adversarial-robustness-toolbox[all]) (2.31.0)\n","Collecting graphviz (from catboost->adversarial-robustness-toolbox[all])\n","  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels->adversarial-robustness-toolbox[all]) (0.5.6)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->adversarial-robustness-toolbox[all]) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->adversarial-robustness-toolbox[all]) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow->adversarial-robustness-toolbox[all]) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->adversarial-robustness-toolbox[all]) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->adversarial-robustness-toolbox[all]) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->adversarial-robustness-toolbox[all]) (18.1.1)\n","Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->adversarial-robustness-toolbox[all]) (0.2.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->adversarial-robustness-toolbox[all]) (3.3.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->adversarial-robustness-toolbox[all]) (3.20.3)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->adversarial-robustness-toolbox[all]) (2.4.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->adversarial-robustness-toolbox[all]) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->adversarial-robustness-toolbox[all]) (0.37.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->adversarial-robustness-toolbox[all]) (1.64.1)\n","Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow->adversarial-robustness-toolbox[all]) (2.15.2)\n","Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->adversarial-robustness-toolbox[all]) (2.15.0)\n","Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->adversarial-robustness-toolbox[all]) (2.15.0)\n","Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons->adversarial-robustness-toolbox[all])\n","  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->adversarial-robustness-toolbox[all]) (0.43.0)\n","Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa->adversarial-robustness-toolbox[all]) (4.2.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet->adversarial-robustness-toolbox[all]) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet->adversarial-robustness-toolbox[all]) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet->adversarial-robustness-toolbox[all]) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet->adversarial-robustness-toolbox[all]) (2024.6.2)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa->adversarial-robustness-toolbox[all]) (1.16.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->adversarial-robustness-toolbox[all]) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->adversarial-robustness-toolbox[all]) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->adversarial-robustness-toolbox[all]) (3.6)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->adversarial-robustness-toolbox[all]) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->adversarial-robustness-toolbox[all]) (3.0.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->adversarial-robustness-toolbox[all]) (2.1.5)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost->adversarial-robustness-toolbox[all]) (8.4.1)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->adversarial-robustness-toolbox[all]) (1.3.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->adversarial-robustness-toolbox[all]) (2.22)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->adversarial-robustness-toolbox[all]) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->adversarial-robustness-toolbox[all]) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->adversarial-robustness-toolbox[all]) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow->adversarial-robustness-toolbox[all]) (1.3.1)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->adversarial-robustness-toolbox[all]) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow->adversarial-robustness-toolbox[all]) (3.2.2)\n","Installing collected packages: pydub, typeguard, kornia-rs, graphviz, ffmpeg-python, cma, tensorflow-addons, resampy, mxnet, catboost, adversarial-robustness-toolbox, kornia\n","  Attempting uninstall: graphviz\n","    Found existing installation: graphviz 0.20.3\n","    Uninstalling graphviz-0.20.3:\n","      Successfully uninstalled graphviz-0.20.3\n","Successfully installed adversarial-robustness-toolbox-1.18.0 catboost-1.2.5 cma-3.3.0 ffmpeg-python-0.2.0 graphviz-0.8.4 kornia-0.7.2 kornia-rs-0.1.3 mxnet-1.9.1 pydub-0.25.1 resampy-0.4.3 tensorflow-addons-0.23.0 typeguard-2.13.3\n"]}],"source":["if IN_COLAB:\n","  !pip install adversarial-robustness-toolbox[all] # installa la libreria ART"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"ghUmWZAtwMua"},"outputs":[],"source":["import art\n","from art.estimators.classification import PyTorchClassifier\n","import torch.nn as nn\n","import numpy as np\n","import torch\n","from art.attacks.evasion import DeepFool"]},{"cell_type":"markdown","metadata":{"id":"zrOf0hk7Q8xC"},"source":["#### 4.1 Load list of images and labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PL5xjjpPyi8r"},"outputs":[],"source":["# Attack.generete vuole dei numpy array, io ho dei tensori\n","# Recupero anche le true label dei 1000 campioni di test\n","#accuracy_clean_data = validate(dataloader, resnet, name_to_id, device)\n","\n","images_list = []\n","labels_list = []\n","for image, label in dataloader:\n","    images_list.append(image.numpy())\n","    labels_list.append(label)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":336,"status":"ok","timestamp":1719504793793,"user":{"displayName":"Kekko Iuorio","userId":"10763977159809291578"},"user_tz":-120},"id":"0h7yA4xc0fu9","outputId":"a1d211bd-9f1c-46a5-f55e-edf55a40b0ee"},"outputs":[{"name":"stdout","output_type":"stream","text":["998\n"]}],"source":["print(len(images_list))"]},{"cell_type":"markdown","metadata":{"id":"g8xjOoEzRKkz"},"source":["#### 4.2 Classifier for NN1"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"atx_XdKKzUNR"},"outputs":[],"source":["from torch import nn, optim\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(resnet.parameters())\n","classifier = PyTorchClassifier(\n","    model=resnet,\n","    clip_values=(0,1),\n","    loss=criterion,\n","    optimizer=optimizer,\n","    input_shape=(3, 160, 160),\n","    nb_classes=8631,\n",")"]},{"cell_type":"markdown","metadata":{"id":"j_4hE8vh5wIA"},"source":["#### 4.3 Funzioni utili"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"b5qX0QFxcNhN"},"outputs":[],"source":["import numpy as np\n","import csv\n","from datetime import datetime\n","import json\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","def compute_perturbation(original_images, adversarial_samples):\n","    perturbations = []\n","    for original_image, adversarial_sample in zip(original_images, adversarial_samples):\n","        perturbation = np.mean(np.abs((np.array(original_image) - np.array(adversarial_sample))))\n","        perturbations.append(perturbation)\n","    return round(float(np.mean(perturbations)),4)\n","\n","\n","def sec_curve(strength_values, accuracies_values, constant_values, strength_name, target_class=None,\n","              attack=None, avg_perturbations=None, accuracy_on_target_class=None, network = \"NN1\"):\n","    fig, ax = plt.subplots()\n","    # Costruisci la stringa per i valori costanti\n","    constant_str = ', '.join([f'{key}: {value}' for key, value in constant_values.items()])\n","    line = ax.plot(np.array(strength_values), np.array(accuracies_values), 'b--', label=f'{network} - {constant_str}')\n","\n","\n","    # Aggiungi i valori costanti come parte della legenda\n","    if target_class:\n","        plt.title('Security Curve for Target Class {}'.format(target_class))\n","    else:\n","        plt.title('Security Curve')\n","\n","    # Aggiungi l'attacco al titolo\n","    if attack:\n","        plt.title(f'{attack} - {plt.gca().get_title()}')\n","\n","    plt.xlabel('Attack strength ({})'.format(strength_name))\n","    plt.ylabel('Accuracy Test')\n","    plt.grid()\n","\n","    # Aggiungi il diagramma a barre di colore arancione per avg_perturbations\n","    if avg_perturbations:\n","        x = np.array(strength_values)\n","        ax2 = ax.twinx()\n","        bar = ax2.bar(x, avg_perturbations, color='orange', alpha=0.5, width=0.01, label='Avg Perturbations')\n","        ax2.set_ylabel('Avg Perturbations')\n","\n","\n","\n","    if accuracy_on_target_class:\n","        # Aggiungi la curva di accuratezza per la classe target\n","        ax.plot(np.array(strength_values), np.array(accuracy_on_target_class), 'r--', label=f'{network} - mis_targ/miss')\n","\n","    # Unisci le linee e le barre in una lista per la legenda\n","    handles, labels = ax.get_legend_handles_labels()\n","    if avg_perturbations:\n","        handles2, labels2 = ax2.get_legend_handles_labels()\n","        handles += handles2\n","        labels += labels2\n","\n","    # Mostra la legenda\n","    #plt.legend(handles, labels, loc='upper right', shadow=True, fontsize='small')\n","    #voglio che la legenda sia fuori dal grafico\n","    plt.legend(handles, labels, loc='upper left', bbox_to_anchor=(1.2, 1), shadow=True, fontsize='small')\n","    plt.show()\n","\n","\n","def save_to_csv(attack_name, targeted, target_class, strength_name, strength_values, accuracy_values, constant_values, avg_perturbations, file_path, accuracy_on_target_class=None):\n","    # Intestazione del file CSV\n","    header = [\"timestamp\", \"attacco\", \"targeted\", \"target_class\", \"strength_name\", \"strength_values\", \"accuracy_values\", \"constant_values\", \"avg_perturbations\", \"accuracy_on_target_class\"]\n","\n","    # Creazione della tupla con i valori da scrivere nel CSV\n","    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","    # Converti le liste di valori in stringhe JSON\n","    strength_values_json = json.dumps(strength_values)\n","    accuracy_values_json = json.dumps(accuracy_values)\n","    avg_perturbations_json = json.dumps(avg_perturbations)  # Converti avg_perturbations in una stringa JSON\n","    # Modifica: formatta strength_values come una lista di numeri invece di una stringa JSON\n","    accuracy_values_list = [float(val) for val in accuracy_values]\n","    # Modifica: converti accuracy_values in una lista di numeri\n","    row = (timestamp, attack_name, targeted, target_class, strength_name, strength_values_json, accuracy_values_list, json.dumps(constant_values), avg_perturbations_json)\n","\n","    # Se accuracy_on_target_class è fornito e non è None, includilo nella tupla\n","    if accuracy_on_target_class is not None:\n","        accuracy_on_target_class_json = json.dumps(accuracy_on_target_class)  # Converti accuracy_on_target_class in una stringa JSON\n","        row += (accuracy_on_target_class_json,)\n","    else:\n","        row += (None,)  # Aggiungi None alla tupla\n","\n","    # Scrittura nel file CSV in modalità append ('a')\n","    with open(file_path, mode='a', newline='') as file:\n","        writer = csv.writer(file)\n","        if file.tell() == 0:  # Se il file è vuoto, scrivi l'intestazione\n","            writer.writerow(header)\n","        writer.writerow(row)\n","\n","\n","def read_csv_and_plot(csv_file_path, network=\"NN1\"):\n","    with open(csv_file_path, mode='r') as file:\n","        reader = csv.DictReader(file)\n","\n","        for row in reader:\n","            timestamp = row[\"timestamp\"]\n","            attack_name = row[\"attacco\"]\n","            targeted = row[\"targeted\"]\n","            target_class = row[\"target_class\"]\n","            strength_name = row[\"strength_name\"]\n","            strength_values = json.loads(row[\"strength_values\"])\n","            accuracy_values = json.loads(row[\"accuracy_values\"])\n","            constant_values = json.loads(row[\"constant_values\"])\n","            avg_perturbations = json.loads(row[\"avg_perturbations\"])\n","            accuracy_on_target_class = json.loads(row[\"accuracy_on_target_class\"]) if row[\"accuracy_on_target_class\"] else None\n","\n","            # Controlla se accuracy_values è una stringa JSON e la elabora correttamente\n","            if isinstance(accuracy_values, str):\n","                accuracy_values = json.loads(accuracy_values)\n","\n","            # Controlla se strength_values è una stringa JSON e la elabora correttamente\n","            if isinstance(strength_values, str):\n","                strength_values = json.loads(strength_values)\n","\n","            # Controlla se strength_values è una stringa JSON e la elabora correttamente\n","            if isinstance(avg_perturbations, str):\n","                avg_perturbations = json.loads(avg_perturbations)\n","\n","            # Chiama la funzione sec_curve per plottare la curva\n","            sec_curve(strength_values, accuracy_values, constant_values, strength_name, target_class, attack_name, avg_perturbations, accuracy_on_target_class, network)"]},{"cell_type":"markdown","metadata":{"id":"GdK0Tgaiy_Er"},"source":["### 2.2 SEC al variare di epsilon e max iter per NN1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fsw78YZAYIjJ"},"outputs":[],"source":["if IN_COLAB:\n","  results_csv = \"/content/drive/Shareddrives/AI4CYBSEC/results/attack_results_NN1_deepfool.csv\"\n","else:\n","  results_csv = \"results\\PGD\\\\attack_results_NN1.csv\"\n","network = \"NN1\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":711926,"status":"ok","timestamp":1719505740826,"user":{"displayName":"Kekko Iuorio","userId":"10763977159809291578"},"user_tz":-120},"id":"x7YrNr2Zy-bM","outputId":"7d3426da-a7f8-4fab-fc91-845204a95efd"},"outputs":[{"name":"stderr","output_type":"stream","text":["Generating DeepFool attacks:  33%|███▎      | 1/3 [04:03<08:06, 243.05s/it]"]},{"name":"stdout","output_type":"stream","text":["pertubation nn1 with eps = 0.1: 0.0018\n"]},{"name":"stderr","output_type":"stream","text":["\rGenerating DeepFool attacks:  67%|██████▋   | 2/3 [07:55<03:57, 237.10s/it]"]},{"name":"stdout","output_type":"stream","text":["pertubation nn1 with eps = 0.2: 0.0019\n"]},{"name":"stderr","output_type":"stream","text":["Generating DeepFool attacks: 100%|██████████| 3/3 [11:51<00:00, 237.18s/it]"]},{"name":"stdout","output_type":"stream","text":["pertubation nn1 with eps = 0.3: 0.0021\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["from tqdm import tqdm\n","if NN1_FLAG:\n","\n","  epsilon_range=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n","  max_iters = [2,5,10]\n","\n","  for max_iter in max_iters:\n","    accuracy_test=list()\n","    accuracy_test.append(acc)\n","    pertubation_list=list()\n","    pertubation_list.append(0)\n","    for epsilon in tqdm (epsilon_range,desc=\"Generating DeepFool attacks\"):\n","      attack = DeepFool(classifier=classifier, epsilon=epsilon, max_iter=max_iter, verbose=False)\n","      sample_indovinati=0\n","      images_adv_list=list()\n","      for img, label in zip(images_list, labels_list):\n","        test_image_adv_nn = attack.generate(img) #ritorna numpy array, per fare la valutazione ho bisogno di tensori\n","        images_adv_list.append(test_image_adv_nn)\n","        test_image_adv_tensor = torch.tensor(test_image_adv_nn)\n","        x_test_adv_pred,_ = make_inference(resnet, test_image_adv_tensor, name_to_id, device)\n","\n","        if x_test_adv_pred[0] == label[0]:\n","                sample_indovinati += 1\n","\n","      #ora ho finito di creare gli avd sample, posso calcolare l'accuracy e perturbation\n","      acc_eps=sample_indovinati/len(dataloader.dataset)\n","      accuracy_test.append(acc_eps)\n","      per=compute_perturbation(images_list, images_adv_list)\n","      pertubation_list.append(per)\n","      #SEC\n","    sec_curve([0]+epsilon_range, accuracy_test,{\"max_iter\":max_iter}, \"eps\", None,\"DeepFool\",pertubation_list,network=\"NN1\")\n","      #Salvataggio risultati\n","    strength_name=\"eps\" #valore che faccio cambiare\n","    constant_values = {\"max_iter\":max_iter}\n","    targeted=False\n","    attack_name=\"DeepFool\"\n","    target_class=None\n","    save_to_csv(attack_name, targeted, target_class, strength_name, [0]+epsilon_range, accuracy_test, constant_values,pertubation_list,results_csv)"]},{"cell_type":"markdown","metadata":{"id":"JYH5VWnJM5xS"},"source":["###5.0 NN2"]},{"cell_type":"markdown","metadata":{"id":"EilczUOFPONb"},"source":["#### 5.1 Load NN2\n"]},{"cell_type":"markdown","metadata":{"id":"Rqrd5cUqbDJl"},"source":["#####5.1.1 Load repo for NN2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LC_OWBxFPNOW"},"outputs":[],"source":["# se non è presente la cartella VGGFACE2_pytorch  clone il repository\n","import os\n","if not os.path.exists('VGGFACE2_pytorch'):\n","    !git clone https://github.com/cydonia999/VGGFace2-pytorch.git\n","    !mv VGGFace2-pytorch VGGFACE2_pytorch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w5BY4c6NPldF"},"outputs":[],"source":["import torch\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":449,"status":"ok","timestamp":1719508491742,"user":{"displayName":"Kekko Iuorio","userId":"10763977159809291578"},"user_tz":-120},"id":"usFbU4jFQmwb","outputId":"5a8a0447-71e4-4f40-98dc-f4b9b35feaa0"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content\n"]}],"source":["%cd /content/drive/Shareddrives/AI4CYBSEC/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oFuA-hVCxmpS"},"outputs":[],"source":["from VGGFace2_pytorch.models import senet as SENet\n","from VGGFace2_pytorch.models.resnet import resnet50 as ResNet\n","from VGGFace2_pytorch import utils\n","from VGGFace2_pytorch.trainer import Validator\n","from torch.utils.data import DataLoader\n","from VGGFace2_pytorch.datasets.vgg_face2 import VGG_Faces2\n","from torch.nn.modules.loss import CrossEntropyLoss"]},{"cell_type":"markdown","metadata":{"id":"sQfUNFuSbJcA"},"source":["#####5.2 Load Model from pickel file"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5247,"status":"ok","timestamp":1719508510927,"user":{"displayName":"Kekko Iuorio","userId":"10763977159809291578"},"user_tz":-120},"id":"v7yZZru-PnAT","outputId":"2a70baa3-f42c-496f-9a8c-30605af4ef50"},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 112, 112]           9,408\n","       BatchNorm2d-2         [-1, 64, 112, 112]             128\n","              ReLU-3         [-1, 64, 112, 112]               0\n","         MaxPool2d-4           [-1, 64, 56, 56]               0\n","            Conv2d-5           [-1, 64, 56, 56]           4,096\n","       BatchNorm2d-6           [-1, 64, 56, 56]             128\n","              ReLU-7           [-1, 64, 56, 56]               0\n","            Conv2d-8           [-1, 64, 56, 56]          36,864\n","       BatchNorm2d-9           [-1, 64, 56, 56]             128\n","             ReLU-10           [-1, 64, 56, 56]               0\n","           Conv2d-11          [-1, 256, 56, 56]          16,384\n","      BatchNorm2d-12          [-1, 256, 56, 56]             512\n","           Conv2d-13             [-1, 16, 1, 1]           4,112\n","             ReLU-14             [-1, 16, 1, 1]               0\n","           Conv2d-15            [-1, 256, 1, 1]           4,352\n","          Sigmoid-16            [-1, 256, 1, 1]               0\n","           Conv2d-17          [-1, 256, 56, 56]          16,384\n","      BatchNorm2d-18          [-1, 256, 56, 56]             512\n","             ReLU-19          [-1, 256, 56, 56]               0\n","       Bottleneck-20          [-1, 256, 56, 56]               0\n","           Conv2d-21           [-1, 64, 56, 56]          16,384\n","      BatchNorm2d-22           [-1, 64, 56, 56]             128\n","             ReLU-23           [-1, 64, 56, 56]               0\n","           Conv2d-24           [-1, 64, 56, 56]          36,864\n","      BatchNorm2d-25           [-1, 64, 56, 56]             128\n","             ReLU-26           [-1, 64, 56, 56]               0\n","           Conv2d-27          [-1, 256, 56, 56]          16,384\n","      BatchNorm2d-28          [-1, 256, 56, 56]             512\n","           Conv2d-29             [-1, 16, 1, 1]           4,112\n","             ReLU-30             [-1, 16, 1, 1]               0\n","           Conv2d-31            [-1, 256, 1, 1]           4,352\n","          Sigmoid-32            [-1, 256, 1, 1]               0\n","             ReLU-33          [-1, 256, 56, 56]               0\n","       Bottleneck-34          [-1, 256, 56, 56]               0\n","           Conv2d-35           [-1, 64, 56, 56]          16,384\n","      BatchNorm2d-36           [-1, 64, 56, 56]             128\n","             ReLU-37           [-1, 64, 56, 56]               0\n","           Conv2d-38           [-1, 64, 56, 56]          36,864\n","      BatchNorm2d-39           [-1, 64, 56, 56]             128\n","             ReLU-40           [-1, 64, 56, 56]               0\n","           Conv2d-41          [-1, 256, 56, 56]          16,384\n","      BatchNorm2d-42          [-1, 256, 56, 56]             512\n","           Conv2d-43             [-1, 16, 1, 1]           4,112\n","             ReLU-44             [-1, 16, 1, 1]               0\n","           Conv2d-45            [-1, 256, 1, 1]           4,352\n","          Sigmoid-46            [-1, 256, 1, 1]               0\n","             ReLU-47          [-1, 256, 56, 56]               0\n","       Bottleneck-48          [-1, 256, 56, 56]               0\n","           Conv2d-49          [-1, 128, 28, 28]          32,768\n","      BatchNorm2d-50          [-1, 128, 28, 28]             256\n","             ReLU-51          [-1, 128, 28, 28]               0\n","           Conv2d-52          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-53          [-1, 128, 28, 28]             256\n","             ReLU-54          [-1, 128, 28, 28]               0\n","           Conv2d-55          [-1, 512, 28, 28]          65,536\n","      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n","           Conv2d-57             [-1, 32, 1, 1]          16,416\n","             ReLU-58             [-1, 32, 1, 1]               0\n","           Conv2d-59            [-1, 512, 1, 1]          16,896\n","          Sigmoid-60            [-1, 512, 1, 1]               0\n","           Conv2d-61          [-1, 512, 28, 28]         131,072\n","      BatchNorm2d-62          [-1, 512, 28, 28]           1,024\n","             ReLU-63          [-1, 512, 28, 28]               0\n","       Bottleneck-64          [-1, 512, 28, 28]               0\n","           Conv2d-65          [-1, 128, 28, 28]          65,536\n","      BatchNorm2d-66          [-1, 128, 28, 28]             256\n","             ReLU-67          [-1, 128, 28, 28]               0\n","           Conv2d-68          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-69          [-1, 128, 28, 28]             256\n","             ReLU-70          [-1, 128, 28, 28]               0\n","           Conv2d-71          [-1, 512, 28, 28]          65,536\n","      BatchNorm2d-72          [-1, 512, 28, 28]           1,024\n","           Conv2d-73             [-1, 32, 1, 1]          16,416\n","             ReLU-74             [-1, 32, 1, 1]               0\n","           Conv2d-75            [-1, 512, 1, 1]          16,896\n","          Sigmoid-76            [-1, 512, 1, 1]               0\n","             ReLU-77          [-1, 512, 28, 28]               0\n","       Bottleneck-78          [-1, 512, 28, 28]               0\n","           Conv2d-79          [-1, 128, 28, 28]          65,536\n","      BatchNorm2d-80          [-1, 128, 28, 28]             256\n","             ReLU-81          [-1, 128, 28, 28]               0\n","           Conv2d-82          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-83          [-1, 128, 28, 28]             256\n","             ReLU-84          [-1, 128, 28, 28]               0\n","           Conv2d-85          [-1, 512, 28, 28]          65,536\n","      BatchNorm2d-86          [-1, 512, 28, 28]           1,024\n","           Conv2d-87             [-1, 32, 1, 1]          16,416\n","             ReLU-88             [-1, 32, 1, 1]               0\n","           Conv2d-89            [-1, 512, 1, 1]          16,896\n","          Sigmoid-90            [-1, 512, 1, 1]               0\n","             ReLU-91          [-1, 512, 28, 28]               0\n","       Bottleneck-92          [-1, 512, 28, 28]               0\n","           Conv2d-93          [-1, 128, 28, 28]          65,536\n","      BatchNorm2d-94          [-1, 128, 28, 28]             256\n","             ReLU-95          [-1, 128, 28, 28]               0\n","           Conv2d-96          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-97          [-1, 128, 28, 28]             256\n","             ReLU-98          [-1, 128, 28, 28]               0\n","           Conv2d-99          [-1, 512, 28, 28]          65,536\n","     BatchNorm2d-100          [-1, 512, 28, 28]           1,024\n","          Conv2d-101             [-1, 32, 1, 1]          16,416\n","            ReLU-102             [-1, 32, 1, 1]               0\n","          Conv2d-103            [-1, 512, 1, 1]          16,896\n","         Sigmoid-104            [-1, 512, 1, 1]               0\n","            ReLU-105          [-1, 512, 28, 28]               0\n","      Bottleneck-106          [-1, 512, 28, 28]               0\n","          Conv2d-107          [-1, 256, 14, 14]         131,072\n","     BatchNorm2d-108          [-1, 256, 14, 14]             512\n","            ReLU-109          [-1, 256, 14, 14]               0\n","          Conv2d-110          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-111          [-1, 256, 14, 14]             512\n","            ReLU-112          [-1, 256, 14, 14]               0\n","          Conv2d-113         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-114         [-1, 1024, 14, 14]           2,048\n","          Conv2d-115             [-1, 64, 1, 1]          65,600\n","            ReLU-116             [-1, 64, 1, 1]               0\n","          Conv2d-117           [-1, 1024, 1, 1]          66,560\n","         Sigmoid-118           [-1, 1024, 1, 1]               0\n","          Conv2d-119         [-1, 1024, 14, 14]         524,288\n","     BatchNorm2d-120         [-1, 1024, 14, 14]           2,048\n","            ReLU-121         [-1, 1024, 14, 14]               0\n","      Bottleneck-122         [-1, 1024, 14, 14]               0\n","          Conv2d-123          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-124          [-1, 256, 14, 14]             512\n","            ReLU-125          [-1, 256, 14, 14]               0\n","          Conv2d-126          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-127          [-1, 256, 14, 14]             512\n","            ReLU-128          [-1, 256, 14, 14]               0\n","          Conv2d-129         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-130         [-1, 1024, 14, 14]           2,048\n","          Conv2d-131             [-1, 64, 1, 1]          65,600\n","            ReLU-132             [-1, 64, 1, 1]               0\n","          Conv2d-133           [-1, 1024, 1, 1]          66,560\n","         Sigmoid-134           [-1, 1024, 1, 1]               0\n","            ReLU-135         [-1, 1024, 14, 14]               0\n","      Bottleneck-136         [-1, 1024, 14, 14]               0\n","          Conv2d-137          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-138          [-1, 256, 14, 14]             512\n","            ReLU-139          [-1, 256, 14, 14]               0\n","          Conv2d-140          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-141          [-1, 256, 14, 14]             512\n","            ReLU-142          [-1, 256, 14, 14]               0\n","          Conv2d-143         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-144         [-1, 1024, 14, 14]           2,048\n","          Conv2d-145             [-1, 64, 1, 1]          65,600\n","            ReLU-146             [-1, 64, 1, 1]               0\n","          Conv2d-147           [-1, 1024, 1, 1]          66,560\n","         Sigmoid-148           [-1, 1024, 1, 1]               0\n","            ReLU-149         [-1, 1024, 14, 14]               0\n","      Bottleneck-150         [-1, 1024, 14, 14]               0\n","          Conv2d-151          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-152          [-1, 256, 14, 14]             512\n","            ReLU-153          [-1, 256, 14, 14]               0\n","          Conv2d-154          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-155          [-1, 256, 14, 14]             512\n","            ReLU-156          [-1, 256, 14, 14]               0\n","          Conv2d-157         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-158         [-1, 1024, 14, 14]           2,048\n","          Conv2d-159             [-1, 64, 1, 1]          65,600\n","            ReLU-160             [-1, 64, 1, 1]               0\n","          Conv2d-161           [-1, 1024, 1, 1]          66,560\n","         Sigmoid-162           [-1, 1024, 1, 1]               0\n","            ReLU-163         [-1, 1024, 14, 14]               0\n","      Bottleneck-164         [-1, 1024, 14, 14]               0\n","          Conv2d-165          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-166          [-1, 256, 14, 14]             512\n","            ReLU-167          [-1, 256, 14, 14]               0\n","          Conv2d-168          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-169          [-1, 256, 14, 14]             512\n","            ReLU-170          [-1, 256, 14, 14]               0\n","          Conv2d-171         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-172         [-1, 1024, 14, 14]           2,048\n","          Conv2d-173             [-1, 64, 1, 1]          65,600\n","            ReLU-174             [-1, 64, 1, 1]               0\n","          Conv2d-175           [-1, 1024, 1, 1]          66,560\n","         Sigmoid-176           [-1, 1024, 1, 1]               0\n","            ReLU-177         [-1, 1024, 14, 14]               0\n","      Bottleneck-178         [-1, 1024, 14, 14]               0\n","          Conv2d-179          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-180          [-1, 256, 14, 14]             512\n","            ReLU-181          [-1, 256, 14, 14]               0\n","          Conv2d-182          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-183          [-1, 256, 14, 14]             512\n","            ReLU-184          [-1, 256, 14, 14]               0\n","          Conv2d-185         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-186         [-1, 1024, 14, 14]           2,048\n","          Conv2d-187             [-1, 64, 1, 1]          65,600\n","            ReLU-188             [-1, 64, 1, 1]               0\n","          Conv2d-189           [-1, 1024, 1, 1]          66,560\n","         Sigmoid-190           [-1, 1024, 1, 1]               0\n","            ReLU-191         [-1, 1024, 14, 14]               0\n","      Bottleneck-192         [-1, 1024, 14, 14]               0\n","          Conv2d-193            [-1, 512, 7, 7]         524,288\n","     BatchNorm2d-194            [-1, 512, 7, 7]           1,024\n","            ReLU-195            [-1, 512, 7, 7]               0\n","          Conv2d-196            [-1, 512, 7, 7]       2,359,296\n","     BatchNorm2d-197            [-1, 512, 7, 7]           1,024\n","            ReLU-198            [-1, 512, 7, 7]               0\n","          Conv2d-199           [-1, 2048, 7, 7]       1,048,576\n","     BatchNorm2d-200           [-1, 2048, 7, 7]           4,096\n","          Conv2d-201            [-1, 128, 1, 1]         262,272\n","            ReLU-202            [-1, 128, 1, 1]               0\n","          Conv2d-203           [-1, 2048, 1, 1]         264,192\n","         Sigmoid-204           [-1, 2048, 1, 1]               0\n","          Conv2d-205           [-1, 2048, 7, 7]       2,097,152\n","     BatchNorm2d-206           [-1, 2048, 7, 7]           4,096\n","            ReLU-207           [-1, 2048, 7, 7]               0\n","      Bottleneck-208           [-1, 2048, 7, 7]               0\n","          Conv2d-209            [-1, 512, 7, 7]       1,048,576\n","     BatchNorm2d-210            [-1, 512, 7, 7]           1,024\n","            ReLU-211            [-1, 512, 7, 7]               0\n","          Conv2d-212            [-1, 512, 7, 7]       2,359,296\n","     BatchNorm2d-213            [-1, 512, 7, 7]           1,024\n","            ReLU-214            [-1, 512, 7, 7]               0\n","          Conv2d-215           [-1, 2048, 7, 7]       1,048,576\n","     BatchNorm2d-216           [-1, 2048, 7, 7]           4,096\n","          Conv2d-217            [-1, 128, 1, 1]         262,272\n","            ReLU-218            [-1, 128, 1, 1]               0\n","          Conv2d-219           [-1, 2048, 1, 1]         264,192\n","         Sigmoid-220           [-1, 2048, 1, 1]               0\n","            ReLU-221           [-1, 2048, 7, 7]               0\n","      Bottleneck-222           [-1, 2048, 7, 7]               0\n","          Conv2d-223            [-1, 512, 7, 7]       1,048,576\n","     BatchNorm2d-224            [-1, 512, 7, 7]           1,024\n","            ReLU-225            [-1, 512, 7, 7]               0\n","          Conv2d-226            [-1, 512, 7, 7]       2,359,296\n","     BatchNorm2d-227            [-1, 512, 7, 7]           1,024\n","            ReLU-228            [-1, 512, 7, 7]               0\n","          Conv2d-229           [-1, 2048, 7, 7]       1,048,576\n","     BatchNorm2d-230           [-1, 2048, 7, 7]           4,096\n","          Conv2d-231            [-1, 128, 1, 1]         262,272\n","            ReLU-232            [-1, 128, 1, 1]               0\n","          Conv2d-233           [-1, 2048, 1, 1]         264,192\n","         Sigmoid-234           [-1, 2048, 1, 1]               0\n","            ReLU-235           [-1, 2048, 7, 7]               0\n","      Bottleneck-236           [-1, 2048, 7, 7]               0\n","       AvgPool2d-237           [-1, 2048, 1, 1]               0\n","          Linear-238                 [-1, 8631]      17,684,919\n","================================================================\n","Total params: 43,723,943\n","Trainable params: 43,723,943\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 274.80\n","Params size (MB): 166.79\n","Estimated Total Size (MB): 442.17\n","----------------------------------------------------------------\n"]}],"source":["import torchsummary\n","\n","model = SENet.senet50(num_classes = 8631, include_top = True)\n","if IN_COLAB:\n","  weights_pickel = \"/content/drive/Shareddrives/AI4CYBSEC/in_progress/senet50_ft_weight.pkl\"\n","else:\n","  weights_pickel = \".\\in_progress\\senet50_ft_weight.pkl\"\n","\n","utils.load_state_dict(model, weights_pickel)\n","\n","device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n","model.to(device)\n","\n","torchsummary.summary(model, (3, 224, 224))"]},{"cell_type":"markdown","metadata":{"id":"7821iznvRCvm"},"source":["####5.3 Utility Function for NN2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Iszxbo-dRO1N"},"outputs":[],"source":["def preprocessing_on_tensor(img_tensor, mean_bgr=np.array([91.4953, 103.8827, 131.0912])):\n","    \"\"\"\n","    Perform preprocessing on the input image tensor for the model.\n","\n","    :param img_tensor: immagine with shape (C, H, W) and values in [0, 1]\n","    :return: immagine normalizzata using mean_bgr with shape (1, C, H, W)\n","\n","    \"\"\"\n","    img = img_tensor.squeeze(0)\n","    img = img.numpy()\n","    img = (img * 255).astype(np.uint8)\n","    mean_bgr = np.array([91.4953,103.8827, 131.0912])\n","    # img è C x H x W --> H x W x C\n","    img = np.transpose(img, (1, 2, 0))\n","    img = img[:, :, ::-1]  # RGB -> BGR\n","    img = img.astype(np.float32)\n","    img -= mean_bgr\n","    img = img.transpose(2, 0, 1)\n","    img_tensor = torch.from_numpy(img).unsqueeze(0)\n","    return img_tensor\n","\n","\n","def make_inference_NN2(model, img_tensor, device, with_preprocessing=True):\n","    \"\"\"\n","    Esegue l'inferenza su un'immagine.\n","    :param model: modello\n","    :param img_tensor: immagine trasformata\n","    :param device: dispositivo\n","    :return: predizione\n","    \"\"\"\n","    if with_preprocessing:\n","        img_tensor = img_tensor.squeeze(0)\n","        img_tensor = preprocessing_on_tensor(img_tensor)\n","    model.eval()\n","    img_tensor = img_tensor.to(device)\n","\n","    with torch.no_grad():\n","        output = model(img_tensor)\n","        pred = torch.argmax(output, dim=1).item()\n","    return pred"]},{"cell_type":"markdown","metadata":{"id":"U7b1FF3jkpFY"},"source":["####5.4 Mapping label for NN2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0bcBNqQRkpoh"},"outputs":[],"source":["import os\n","def create_image_list_file(root_dir, output_file, ext = '.jpg'):\n","\n","    image_paths = []\n","\n","    for class_id in os.listdir(root_dir):\n","        class_dir = os.path.join(root_dir, class_id)\n","\n","        if os.path.isdir(class_dir):\n","\n","            for filename in os.listdir(class_dir):\n","\n","                if filename.endswith(ext):\n","                    image_path = f\"{os.path.basename(root_dir)}/{class_id}/{filename}\"\n","                    image_paths.append(image_path)\n","\n","    with open(output_file, 'w') as f:\n","        for image_path in image_paths:\n","            f.write(image_path + '\\n')\n","\n","    print(f\"File di output creato con successo: {output_file}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10493,"status":"ok","timestamp":1719508531810,"user":{"displayName":"Kekko Iuorio","userId":"10763977159809291578"},"user_tz":-120},"id":"ZySZAaP9k3zR","outputId":"e0881cb5-7ba2-4a50-e308-41b21f1d991c"},"outputs":[{"name":"stdout","output_type":"stream","text":["File di output creato con successo: /content/drive/Shareddrives/AI4CYBSEC/image_list_file_NN2.txt\n"]},{"name":"stderr","output_type":"stream","text":["/content/drive/Shareddrives/AI4CYBSEC/VGGFace2_pytorch/utils.py:37: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n","  df = pd.read_csv(identity_list, sep=',\\s+', quoting=csv.QUOTE_ALL, encoding=\"utf-8\")\n"]}],"source":["test_set_NN2 = \"test_set_MTCNN_NN2\"\n","if IN_COLAB:\n","    output_file = '/content/drive/Shareddrives/AI4CYBSEC/image_list_file_NN2.txt'\n","    meta_file = \"/content/drive/Shareddrives/AI4CYBSEC/face_dataset/identity_meta.csv\"\n","else:\n","    output_file = 'image_list_file_NN2.txt'\n","    meta_file = \".\\\\face_dataset\\identity_meta.csv\"\n","root_dir = os.path.join(path_dataset,test_set_NN2)\n","create_image_list_file(root_dir, output_file)\n","id_label_dict = utils.get_id_label_map(meta_file)"]},{"cell_type":"markdown","metadata":{"id":"VCPCVLCbSfT7"},"source":["####5.6 Validate clean data NN2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":376},"executionInfo":{"elapsed":224774,"status":"error","timestamp":1719508758647,"user":{"displayName":"Kekko Iuorio","userId":"10763977159809291578"},"user_tz":-120},"id":"8EMpqdQ3Sebq","outputId":"f6d3a7ee-13ec-4785-9d18-0fd04d9fbda0"},"outputs":[{"name":"stderr","output_type":"stream","text":["\rValid iteration=0 epoch=0:   0%|                        | 0/998 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["Test: [0/998/0]\tepoch: 0\titer: 0\tTime: 1.191 (1.191)\tLoss: 0.0000 (0.0000)\tPrec@1: 0.000 (0.000)\tPrec@5: 0.000 (0.000)\t\n"]},{"name":"stderr","output_type":"stream","text":[]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-46-a29694a0d49f>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m           )\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0macc_NN2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Acc on clean data: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_NN2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/Shareddrives/AI4CYBSEC/VGGFace2_pytorch/trainer.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(self, make_inference)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         for batch_idx, (imgs, target, img_files, class_ids) in tqdm.tqdm(\n\u001b[0m\u001b[1;32m     83\u001b[0m                 \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 desc='Valid iteration={} epoch={}'.format(self.iteration, self.epoch), ncols=80, leave=False):\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/Shareddrives/AI4CYBSEC/VGGFace2_pytorch/datasets/vgg_face2.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mimg_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3254\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3256\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3258\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["val_dataset = VGG_Faces2(\"./face_dataset\", output_file, id_label_dict, split = 'valid') # dataset definito nella repo\n","val_loader = DataLoader(val_dataset, batch_size = 1)\n","validator = Validator(\n","            cmd = \"test\",\n","            cuda = True,\n","            model = model,\n","            criterion = CrossEntropyLoss(),\n","            val_loader = val_loader,\n","            log_file = \"./log_file\",\n","            print_freq = 1000,\n","          )\n","\n","acc_NN2 = validator.validate()\n","print(\"Acc on clean data: \", acc_NN2)"]},{"cell_type":"markdown","metadata":{"id":"swfxzHbjSupr"},"source":["####5.7 Validate NN2 on adv samples\n","Per valutare la trasferibilità degli attacchi. Attacco gray box."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RhPRYo-1TGqi"},"outputs":[],"source":["from torch import nn, optim\n","\n","input_shape = (3,160,160)\n","nb_classes = 8631\n","## Attack Gray Box\n","# set device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","loss = nn.CrossEntropyLoss()\n","#loss = nn.TripletMarginLoss()\n","optimizer = optim.Adam(resnet.parameters())\n","classifier = PyTorchClassifier(model=resnet, loss=loss, input_shape=input_shape, nb_classes=nb_classes, optimizer=optimizer, clip_values=(0, 1))"]},{"cell_type":"markdown","metadata":{"id":"eQNKe_NgTrow"},"source":["#####5.7.1 Load list of images and labels"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":485319,"status":"ok","timestamp":1719509263134,"user":{"displayName":"Kekko Iuorio","userId":"10763977159809291578"},"user_tz":-120},"id":"C1mdDyjITzA6","outputId":"7fc2e773-bee7-4b16-fdde-65d1b55dcb9f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset length: 998\n"]}],"source":["data_transform = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor()\n","  ])\n","if IN_COLAB:\n","  test_set_path = \"/content/drive/Shareddrives/AI4CYBSEC/face_dataset/test_set_MTCNN_NN2\"\n","else:\n","  test_set_path = \"./face_dataset/test_set_MTCNN_NN2\"\n","\n","# Define dataset\n","dataset = VGGFace2Dataset(root_dir=test_set_path, image_size=(224,224), transform=data_transform)\n","\n","# Check the length of the dataset\n","print(\"Dataset length:\", len(dataset))\n","\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n","\n","images_list = []\n","labels_list = []\n","images_adv_list = []\n","for image, label in dataloader:\n","    image_numpy = image.numpy()\n","    images_list.append(image_numpy)\n","    labels_list.append(label)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LEGQPso3T2hF"},"outputs":[],"source":["\n","network=\"NN2\"\n","if IN_COLAB:\n","  results_csv = \"/content/drive/Shareddrives/AI4CYBSEC/results/attack_results_NN2_deepfool.csv\"\n","else:\n","  results_csv = \"results\\PGD\\\\attack_results_NN2.csv\"\n","attack_name=\"DeepFool\""]},{"cell_type":"markdown","metadata":{"id":"AMvOjYOMUcuW"},"source":["#####5.7.2 SEC at the change of esp and max_iter on NN2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j_Fer3PKoXNp"},"outputs":[],"source":["from tqdm import tqdm\n","from art.attacks.evasion import ProjectedGradientDescentPyTorch\n","import time"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1163190,"status":"ok","timestamp":1719511539812,"user":{"displayName":"Kekko Iuorio","userId":"10763977159809291578"},"user_tz":-120},"id":"fRnGzFx4UgFY","outputId":"ffa4b59d-90f2-491b-a2e4-4fad67a8cf93"},"outputs":[{"name":"stderr","output_type":"stream","text":["Generating DeepFool attacks:  20%|██        | 1/5 [03:51<15:25, 231.34s/it]"]},{"name":"stdout","output_type":"stream","text":["per eps = 0.5 ottengo perturbazione pari a 0.001\n"]},{"name":"stderr","output_type":"stream","text":["\rGenerating DeepFool attacks:  40%|████      | 2/5 [07:45<11:39, 233.13s/it]"]},{"name":"stdout","output_type":"stream","text":["per eps = 0.6 ottengo perturbazione pari a 0.0011\n"]},{"name":"stderr","output_type":"stream","text":["\rGenerating DeepFool attacks:  60%|██████    | 3/5 [11:35<07:42, 231.42s/it]"]},{"name":"stdout","output_type":"stream","text":["per eps = 0.7 ottengo perturbazione pari a 0.0011\n"]},{"name":"stderr","output_type":"stream","text":["\rGenerating DeepFool attacks:  80%|████████  | 4/5 [15:26<03:51, 231.55s/it]"]},{"name":"stdout","output_type":"stream","text":["per eps = 0.8 ottengo perturbazione pari a 0.0012\n"]},{"name":"stderr","output_type":"stream","text":["Generating DeepFool attacks: 100%|██████████| 5/5 [19:22<00:00, 232.57s/it]"]},{"name":"stdout","output_type":"stream","text":["per eps = 0.9 ottengo perturbazione pari a 0.0013\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["if NN2_FLAG:\n","  epsilon_range=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n","  max_iters = [2,5,10]\n","\n","  for max_iter in max_iters:\n","    accuracy_test=list()\n","    accuracy_test.append(acc_NN2)\n","    pertubation_list=list()\n","    pertubation_list.append(0)\n","    for epsilon in tqdm (epsilon_range,desc=\"Generating DeepFool attacks\"):\n","      attack = DeepFool(classifier=classifier, epsilon=epsilon, max_iter=max_iter, verbose=False)\n","      sample_indovinati=0\n","      images_adv_list=list()\n","\n","      for img, label in zip(images_list, labels_list):\n","        test_image_adv_nn = attack.generate(img) #ritorna numpy array, per fare la valutazione ho bisogno di tensori\n","        images_adv_list.append(test_image_adv_nn)\n","        test_image_adv_tensor = torch.tensor(test_image_adv_nn)\n","        x_test_adv_pred,_ = make_inference_NN2(model, test_image_adv_tensor, device,with_preprocessing=True)\n","\n","        if x_test_adv_pred[0] == id_label_dict[label[0]]:\n","          sample_indovinati += 1\n","\n","      #ora ho finito di creare gli avd sample, posso calcolare l'accuracy e perturbation\n","      acc=sample_indovinati/len(dataloader.dataset)\n","      accuracy_test.append(acc)\n","      per=compute_perturbation(images_list, images_adv_list)\n","      pertubation_list.append(per)\n","    #SEC\n","    sec_curve([0]+epsilon_range, accuracy_test,{\"max_iter\":max_iter}, \"eps\", None,\"DeepFool\",pertubation_list,network=\"NN2\")\n","    #save results\n","    strength_name=\"eps\" #valore che faccio cambiare\n","    constant_values = {\"max_iter\":max_iter}\n","    targeted=False\n","    target_class=None\n","    save_to_csv(attack_name, targeted, target_class, strength_name,[0]+epsilon_range, accuracy_test,constant_values,pertubation_list, results_csv)"]},{"cell_type":"markdown","metadata":{"id":"ryxP3DR4bTva"},"source":["####6.0 DeepFool NN1 with defense"]},{"cell_type":"markdown","metadata":{"id":"VebA38BCbia9"},"source":["#####6.1 Load Robus Detector for pre-processing"]},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3492,"status":"ok","timestamp":1719502120636,"user":{"displayName":"Kekko Iuorio","userId":"10763977159809291578"},"user_tz":-120},"id":"RNaPe4UIbnv8","outputId":"b9022456-f16a-492e-fe3f-108639c73549"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Program_for_ML\\envs\\AI4CyberSec\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","c:\\Program_for_ML\\envs\\AI4CyberSec\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 32, 80, 80]             864\n","       BatchNorm2d-2           [-1, 32, 80, 80]              64\n","             ReLU6-3           [-1, 32, 80, 80]               0\n","            Conv2d-4           [-1, 32, 80, 80]             288\n","       BatchNorm2d-5           [-1, 32, 80, 80]              64\n","             ReLU6-6           [-1, 32, 80, 80]               0\n","            Conv2d-7           [-1, 16, 80, 80]             512\n","       BatchNorm2d-8           [-1, 16, 80, 80]              32\n","  InvertedResidual-9           [-1, 16, 80, 80]               0\n","           Conv2d-10           [-1, 96, 80, 80]           1,536\n","      BatchNorm2d-11           [-1, 96, 80, 80]             192\n","            ReLU6-12           [-1, 96, 80, 80]               0\n","           Conv2d-13           [-1, 96, 40, 40]             864\n","      BatchNorm2d-14           [-1, 96, 40, 40]             192\n","            ReLU6-15           [-1, 96, 40, 40]               0\n","           Conv2d-16           [-1, 24, 40, 40]           2,304\n","      BatchNorm2d-17           [-1, 24, 40, 40]              48\n"," InvertedResidual-18           [-1, 24, 40, 40]               0\n","           Conv2d-19          [-1, 144, 40, 40]           3,456\n","      BatchNorm2d-20          [-1, 144, 40, 40]             288\n","            ReLU6-21          [-1, 144, 40, 40]               0\n","           Conv2d-22          [-1, 144, 40, 40]           1,296\n","      BatchNorm2d-23          [-1, 144, 40, 40]             288\n","            ReLU6-24          [-1, 144, 40, 40]               0\n","           Conv2d-25           [-1, 24, 40, 40]           3,456\n","      BatchNorm2d-26           [-1, 24, 40, 40]              48\n"," InvertedResidual-27           [-1, 24, 40, 40]               0\n","           Conv2d-28          [-1, 144, 40, 40]           3,456\n","      BatchNorm2d-29          [-1, 144, 40, 40]             288\n","            ReLU6-30          [-1, 144, 40, 40]               0\n","           Conv2d-31          [-1, 144, 20, 20]           1,296\n","      BatchNorm2d-32          [-1, 144, 20, 20]             288\n","            ReLU6-33          [-1, 144, 20, 20]               0\n","           Conv2d-34           [-1, 32, 20, 20]           4,608\n","      BatchNorm2d-35           [-1, 32, 20, 20]              64\n"," InvertedResidual-36           [-1, 32, 20, 20]               0\n","           Conv2d-37          [-1, 192, 20, 20]           6,144\n","      BatchNorm2d-38          [-1, 192, 20, 20]             384\n","            ReLU6-39          [-1, 192, 20, 20]               0\n","           Conv2d-40          [-1, 192, 20, 20]           1,728\n","      BatchNorm2d-41          [-1, 192, 20, 20]             384\n","            ReLU6-42          [-1, 192, 20, 20]               0\n","           Conv2d-43           [-1, 32, 20, 20]           6,144\n","      BatchNorm2d-44           [-1, 32, 20, 20]              64\n"," InvertedResidual-45           [-1, 32, 20, 20]               0\n","           Conv2d-46          [-1, 192, 20, 20]           6,144\n","      BatchNorm2d-47          [-1, 192, 20, 20]             384\n","            ReLU6-48          [-1, 192, 20, 20]               0\n","           Conv2d-49          [-1, 192, 20, 20]           1,728\n","      BatchNorm2d-50          [-1, 192, 20, 20]             384\n","            ReLU6-51          [-1, 192, 20, 20]               0\n","           Conv2d-52           [-1, 32, 20, 20]           6,144\n","      BatchNorm2d-53           [-1, 32, 20, 20]              64\n"," InvertedResidual-54           [-1, 32, 20, 20]               0\n","           Conv2d-55          [-1, 192, 20, 20]           6,144\n","      BatchNorm2d-56          [-1, 192, 20, 20]             384\n","            ReLU6-57          [-1, 192, 20, 20]               0\n","           Conv2d-58          [-1, 192, 10, 10]           1,728\n","      BatchNorm2d-59          [-1, 192, 10, 10]             384\n","            ReLU6-60          [-1, 192, 10, 10]               0\n","           Conv2d-61           [-1, 64, 10, 10]          12,288\n","      BatchNorm2d-62           [-1, 64, 10, 10]             128\n"," InvertedResidual-63           [-1, 64, 10, 10]               0\n","           Conv2d-64          [-1, 384, 10, 10]          24,576\n","      BatchNorm2d-65          [-1, 384, 10, 10]             768\n","            ReLU6-66          [-1, 384, 10, 10]               0\n","           Conv2d-67          [-1, 384, 10, 10]           3,456\n","      BatchNorm2d-68          [-1, 384, 10, 10]             768\n","            ReLU6-69          [-1, 384, 10, 10]               0\n","           Conv2d-70           [-1, 64, 10, 10]          24,576\n","      BatchNorm2d-71           [-1, 64, 10, 10]             128\n"," InvertedResidual-72           [-1, 64, 10, 10]               0\n","           Conv2d-73          [-1, 384, 10, 10]          24,576\n","      BatchNorm2d-74          [-1, 384, 10, 10]             768\n","            ReLU6-75          [-1, 384, 10, 10]               0\n","           Conv2d-76          [-1, 384, 10, 10]           3,456\n","      BatchNorm2d-77          [-1, 384, 10, 10]             768\n","            ReLU6-78          [-1, 384, 10, 10]               0\n","           Conv2d-79           [-1, 64, 10, 10]          24,576\n","      BatchNorm2d-80           [-1, 64, 10, 10]             128\n"," InvertedResidual-81           [-1, 64, 10, 10]               0\n","           Conv2d-82          [-1, 384, 10, 10]          24,576\n","      BatchNorm2d-83          [-1, 384, 10, 10]             768\n","            ReLU6-84          [-1, 384, 10, 10]               0\n","           Conv2d-85          [-1, 384, 10, 10]           3,456\n","      BatchNorm2d-86          [-1, 384, 10, 10]             768\n","            ReLU6-87          [-1, 384, 10, 10]               0\n","           Conv2d-88           [-1, 64, 10, 10]          24,576\n","      BatchNorm2d-89           [-1, 64, 10, 10]             128\n"," InvertedResidual-90           [-1, 64, 10, 10]               0\n","           Conv2d-91          [-1, 384, 10, 10]          24,576\n","      BatchNorm2d-92          [-1, 384, 10, 10]             768\n","            ReLU6-93          [-1, 384, 10, 10]               0\n","           Conv2d-94          [-1, 384, 10, 10]           3,456\n","      BatchNorm2d-95          [-1, 384, 10, 10]             768\n","            ReLU6-96          [-1, 384, 10, 10]               0\n","           Conv2d-97           [-1, 96, 10, 10]          36,864\n","      BatchNorm2d-98           [-1, 96, 10, 10]             192\n"," InvertedResidual-99           [-1, 96, 10, 10]               0\n","          Conv2d-100          [-1, 576, 10, 10]          55,296\n","     BatchNorm2d-101          [-1, 576, 10, 10]           1,152\n","           ReLU6-102          [-1, 576, 10, 10]               0\n","          Conv2d-103          [-1, 576, 10, 10]           5,184\n","     BatchNorm2d-104          [-1, 576, 10, 10]           1,152\n","           ReLU6-105          [-1, 576, 10, 10]               0\n","          Conv2d-106           [-1, 96, 10, 10]          55,296\n","     BatchNorm2d-107           [-1, 96, 10, 10]             192\n","InvertedResidual-108           [-1, 96, 10, 10]               0\n","          Conv2d-109          [-1, 576, 10, 10]          55,296\n","     BatchNorm2d-110          [-1, 576, 10, 10]           1,152\n","           ReLU6-111          [-1, 576, 10, 10]               0\n","          Conv2d-112          [-1, 576, 10, 10]           5,184\n","     BatchNorm2d-113          [-1, 576, 10, 10]           1,152\n","           ReLU6-114          [-1, 576, 10, 10]               0\n","          Conv2d-115           [-1, 96, 10, 10]          55,296\n","     BatchNorm2d-116           [-1, 96, 10, 10]             192\n","InvertedResidual-117           [-1, 96, 10, 10]               0\n","          Conv2d-118          [-1, 576, 10, 10]          55,296\n","     BatchNorm2d-119          [-1, 576, 10, 10]           1,152\n","           ReLU6-120          [-1, 576, 10, 10]               0\n","          Conv2d-121            [-1, 576, 5, 5]           5,184\n","     BatchNorm2d-122            [-1, 576, 5, 5]           1,152\n","           ReLU6-123            [-1, 576, 5, 5]               0\n","          Conv2d-124            [-1, 160, 5, 5]          92,160\n","     BatchNorm2d-125            [-1, 160, 5, 5]             320\n","InvertedResidual-126            [-1, 160, 5, 5]               0\n","          Conv2d-127            [-1, 960, 5, 5]         153,600\n","     BatchNorm2d-128            [-1, 960, 5, 5]           1,920\n","           ReLU6-129            [-1, 960, 5, 5]               0\n","          Conv2d-130            [-1, 960, 5, 5]           8,640\n","     BatchNorm2d-131            [-1, 960, 5, 5]           1,920\n","           ReLU6-132            [-1, 960, 5, 5]               0\n","          Conv2d-133            [-1, 160, 5, 5]         153,600\n","     BatchNorm2d-134            [-1, 160, 5, 5]             320\n","InvertedResidual-135            [-1, 160, 5, 5]               0\n","          Conv2d-136            [-1, 960, 5, 5]         153,600\n","     BatchNorm2d-137            [-1, 960, 5, 5]           1,920\n","           ReLU6-138            [-1, 960, 5, 5]               0\n","          Conv2d-139            [-1, 960, 5, 5]           8,640\n","     BatchNorm2d-140            [-1, 960, 5, 5]           1,920\n","           ReLU6-141            [-1, 960, 5, 5]               0\n","          Conv2d-142            [-1, 160, 5, 5]         153,600\n","     BatchNorm2d-143            [-1, 160, 5, 5]             320\n","InvertedResidual-144            [-1, 160, 5, 5]               0\n","          Conv2d-145            [-1, 960, 5, 5]         153,600\n","     BatchNorm2d-146            [-1, 960, 5, 5]           1,920\n","           ReLU6-147            [-1, 960, 5, 5]               0\n","          Conv2d-148            [-1, 960, 5, 5]           8,640\n","     BatchNorm2d-149            [-1, 960, 5, 5]           1,920\n","           ReLU6-150            [-1, 960, 5, 5]               0\n","          Conv2d-151            [-1, 320, 5, 5]         307,200\n","     BatchNorm2d-152            [-1, 320, 5, 5]             640\n","InvertedResidual-153            [-1, 320, 5, 5]               0\n","          Conv2d-154           [-1, 1280, 5, 5]         409,600\n","     BatchNorm2d-155           [-1, 1280, 5, 5]           2,560\n","           ReLU6-156           [-1, 1280, 5, 5]               0\n","         Dropout-157                 [-1, 1280]               0\n","          Linear-158                    [-1, 2]           2,562\n","================================================================\n","Total params: 2,226,434\n","Trainable params: 2,226,434\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.29\n","Forward/backward pass size (MB): 77.99\n","Params size (MB): 8.49\n","Estimated Total Size (MB): 86.78\n","----------------------------------------------------------------\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","if IN_COLAB:\n","    directory = \"/content/drive/Shareddrives/AI4CYBSEC/models\"\n","else:\n","    directory = \".\\models\"\n","def load_model(model, model_path):\n","    model.load_state_dict(torch.load(model_path))\n","    model.eval()\n","    return model\n","\n","# Definisci il modello mobilenet_v2\n","model = models.mobilenet_v2(pretrained=True)\n","\n","# Sostituisci il classificatore dell'ultimo layer con un nuovo classificatore\n","model.classifier[1] = nn.Linear(model.classifier[1].in_features, 2)\n","\n","model = model.to(device)\n","\n","# carica i pesi del modello addestrato\n","defence = load_model(model, os.path.join(directory,'mobilenetv2_best_binary_classifier.pth'))\n","\n","import torchsummary\n","\n","# Stampa un riassunto del modello\n","torchsummary.summary(defence, (3, 160, 160))"]},{"cell_type":"markdown","metadata":{"id":"PbrI5k1Cb5ve"},"source":["#####6.2 Utility Functions"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"J_ygiCAYb7-j"},"outputs":[],"source":["def make_inference_defence(model, img_tensor, device):\n","    #img must be a tensor with shape (N, C, H, W)\n","    model.eval()\n","    img_tensor = img_tensor.to(device)\n","    with torch.no_grad():\n","        outputs = model(img_tensor)\n","        _, predicted = torch.max(outputs, 1)\n","\n","    return predicted.item()\n","\n","def make_inference_NN1_with_defense(model,img_tensor, name_to_id, defense_model, device, isClean ):\n","    model.to(device)\n","    defense_model.to(device)\n","    prediction_defense = make_inference_defence(defense_model, img_tensor, device)\n","    if prediction_defense == 1:\n","        if not isClean:\n","            return 1, None\n","        return 0 ,None\n","\n","    return make_inference(model, img_tensor, name_to_id, device)\n","\n","def validate_with_defence(dataloader, model, name_to_id, device, defence_model, clean_data = True):\n","    \"\"\"\n","    Validates a model on a dataset and returns the accuracy.\n","\n","    Args:\n","        dataset: Dataloader to validate the model on.\n","        model: Model to validate.\n","        device: Device to perform inference on.\n","\n","    Returns:\n","        accuracy: Accuracy of the model on the dataset.\n","    \"\"\"\n","    model.to(device)\n","    model.eval()\n","    correct_predictions = 0\n","    total_samples = len(dataloader) * dataloader.batch_size\n","    num_skipped_samples = 0\n","    with torch.no_grad():  # Disable gradient calculation\n","        for images, labels in tqdm(dataloader, desc=\"Validating model\"):\n","            predicted_classes, _= make_inference_NN1_with_defense(model, images, name_to_id, defence_model, device, clean_data)\n","            if predicted_classes == 1:# significa che ho predetto come adv un campione  adv\n","                num_skipped_samples += 1\n","                correct_predictions += 1\n","                continue\n","            elif predicted_classes == 0:# significa che ho predetto come adv un campione  clean\n","                num_skipped_samples += 1\n","                continue\n","            # DATO CHE C'è il continue è come se avessi un else:\n","            correct_predictions += sum(pred == label for pred, label in zip(predicted_classes, labels))\n","\n","\n","    # Compute accuracy\n","    accuracy = correct_predictions / total_samples\n","    return accuracy, num_skipped_samples"]},{"cell_type":"markdown","metadata":{"id":"RI60sZpUcA8V"},"source":["#####6.3 Validation on clean data with defense"]},{"cell_type":"code","execution_count":58,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":317,"status":"ok","timestamp":1719502155016,"user":{"displayName":"Kekko Iuorio","userId":"10763977159809291578"},"user_tz":-120},"id":"eyZlJzpecEXg","outputId":"e2e64b84-157f-45ba-f81d-20dbd6b2420f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset length: 998\n"]},{"name":"stderr","output_type":"stream","text":["Validating model: 100%|██████████| 998/998 [00:22<00:00, 43.56it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n"," Accuracy without defense 0.8346693386773547\n"]},{"name":"stderr","output_type":"stream","text":["Validating model: 100%|██████████| 998/998 [00:34<00:00, 29.26it/s]"]},{"name":"stdout","output_type":"stream","text":["\n"," Accuracy with defense 0.8316633266533067\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Create transform for image resizing and normalization\n","\n","data_transform = transforms.Compose([\n","    transforms.Resize((160, 160)),\n","    transforms.ToTensor()\n","])\n","if IN_COLAB:\n","  test_set_path = \"/content/drive/Shareddrives/AI4CYBSEC/face_dataset/test_set_MTCNN\"\n","else:\n","  test_set_path = \"./face_dataset/test_set_MTCNN\"\n","  #test_set_path = \"G:\\Drive condivisi\\AI4CYBSEC\\\\face_dataset\\\\adversarial_samples\\DeepFool_v3\"\n","\n","# Define dataset\n","dataset = VGGFace2Dataset(root_dir=test_set_path, transform=data_transform)\n","\n","# Check the length of the dataset\n","print(\"Dataset length:\", len(dataset))\n","\n","# Create DataLoader\n","batch_size = 1\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n","acc = validate(dataloader, resnet, name_to_id, device)\n","print(\"\\n Accuracy without defense \"+str(acc))\n","\n","acc_with_defence, num_skipped_samples = validate_with_defence(dataloader, resnet, name_to_id, device, defence, clean_data = True)\n","print(\"\\n Accuracy with defense \"+str(acc_with_defence))\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Numero di campioni saltati:  8\n"]}],"source":["print(\"Numero di campioni saltati: \", num_skipped_samples)"]},{"cell_type":"markdown","metadata":{"id":"z-SRWFf6gNSk"},"source":["##### 6.4 Deepfool attack with defence --NN1"]},{"cell_type":"markdown","metadata":{"id":"3jdZ4AyegToB"},"source":["######6.4.1 Load list of images and labels"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"PhzqJ4wecW30"},"outputs":[{"name":"stdout","output_type":"stream","text":["(1, 3, 160, 160)\n"]}],"source":["images_list = []\n","labels_list = []\n","images_adv_list = []\n","for image, label in dataloader:\n","    # Effettua le predizioni del modello\n","    # image numpy on device\n","    image_numpy = image.numpy()\n","    images_list.append(image_numpy)\n","    labels_list.append(label)\n","\n","print(images_list[0].shape)"]},{"cell_type":"markdown","metadata":{"id":"VQgbhqcHc6n1"},"source":["######6.4.2 Perform attack"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"Pbrnk7RWclnd"},"outputs":[],"source":["if IN_COLAB:\n","  results_csv = \"/content/drive/Shareddrives/AI4CYBSEC/results/attack_results_NN1_with_defense.csv\"\n","else:\n","  results_csv = \"results\\\\attack_results_NN1_with_defense.csv\"\n","\n","network = \"NN1_with_defense\"\n","attack_name=\"DeepFool\"\n","targeted=False\n","target_class=None\n","NN1_WITH_DEFENCE=True"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"try_mdQidA4M"},"outputs":[],"source":["from tqdm import tqdm\n","from art.attacks.evasion import ProjectedGradientDescentPyTorch\n","import time"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"dfOfHAKadLRE"},"outputs":[],"source":["from torch import nn, optim\n","input_shape = (3,160,160)\n","nb_classes = 8631\n","## Attack Gray Box\n","# set device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","loss = nn.CrossEntropyLoss()\n","#loss = nn.TripletMarginLoss()\n","optimizer = optim.Adam(resnet.parameters())\n","classifier = PyTorchClassifier(model=resnet, loss=loss, input_shape=input_shape, nb_classes=nb_classes, optimizer=optimizer, clip_values=(0, 1))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1285246,"status":"ok","timestamp":1719508260470,"user":{"displayName":"Kekko Iuorio","userId":"10763977159809291578"},"user_tz":-120},"id":"od4mfWR2db11","outputId":"bc26560c-7e31-4fb3-988b-2c39c98f3ead"},"outputs":[],"source":["if NN1_WITH_DEFENCE:\n","  epsilon_range=[0.1]\n","  max_iters = [2]\n","  for max_iter in max_iters:\n","    accuracy_test=[]\n","    accuracy_test.append(0.830)\n","    pertubation_list=[]\n","    pertubation_list.append(0)\n","    for epsilon in epsilon_range:\n","      attack = DeepFool(classifier=classifier, epsilon=epsilon, max_iter=max_iter, verbose=False)\n","      sample_indovinati=0\n","      num_skipped_samples=0\n","      images_adv_list=[]\n","      count_image=0\n","      for img, label in tqdm(zip(images_list, labels_list),total=len(images_list), desc=f\"Generating DeepFool attacks with {epsilon} epsilon and {max_iter} max_iter\"):\n","        test_image_adv_nn = attack.generate(img) #ritorna numpy array, per fare la valutazione ho bisogno di tensori\n","        images_adv_list.append(test_image_adv_nn)\n","        test_image_adv_tensor = torch.tensor(test_image_adv_nn)\n","        x_test_adv_pred,_ = make_inference_NN1_with_defense(resnet, test_image_adv_tensor, name_to_id,defence, device,isClean=False)\n","        if x_test_adv_pred == 1:# significa che ho predetto come adv un campione  adv\n","          num_skipped_samples += 1\n","          sample_indovinati += 1\n","          continue\n","        elif x_test_adv_pred == 0:# significa che ho predetto come adv un campione  clean\n","          num_skipped_samples += 1\n","          continue\n","        if x_test_adv_pred[0] == label[0]:\n","          sample_indovinati +=1\n","        count_image=count_image + 1\n","        if count_image == 200:\n","          break\n","      #ora ho finito di creare gli avd sample, posso calcolare l'accuracy e perturbation\n","      acc_eps=sample_indovinati/200\n","      accuracy_test.append(acc_eps)\n","      per=compute_perturbation(images_list[0:200], images_adv_list)\n","      pertubation_list.append(per)\n","      print(\"Values for eps = \",epsilon)\n","      print(\"accuracy: \", sample_indovinati/len(images_list))\n","      print(\"Number of skipped samples: \", num_skipped_samples)\n","      print(\"pertubation: \", per)\n","\n","    #sec_curve([0] + epsilon_range, accuracy_test,{\"max_iter\":max_iter}, \"eps\", None,\"DeepFool\",pertubation_list,None, network)\n","    #Salvataggio risultati\n","    #strength_name=\"eps\" #valore che faccio cambiare\n","    #constant_values = {\"max_iter\":max_iter}\n","    #save_to_csv(attack_name, targeted, target_class, strength_name,[0]+ epsilon_range, accuracy_test, constant_values,pertubation_list,results_csv)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"widgets":{"application/vnd.jupyter.widget-state+json":{"18d5e290aec14797878c9fa7204db755":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"496859beee7a4269a660f6825ec0a005":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4cb6d5cdc91d4883b5fb4901599fca69":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"65ecd141d8084cfbb3e82e6527b156d2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"770ea654644745bb93ae93e1783de921":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7735c539218e46d99592c99166f64b2a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a6b3bc121c4f4a4499146e843fc0450b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a770f3c95cb24128a38346aa8e2e633b","placeholder":"​","style":"IPY_MODEL_496859beee7a4269a660f6825ec0a005","value":" 107M/107M [00:00&lt;00:00, 296MB/s]"}},"a770f3c95cb24128a38346aa8e2e633b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b668e3fa0c5c4bc5b6a5d8a7252cbb95":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_770ea654644745bb93ae93e1783de921","max":111898327,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4cb6d5cdc91d4883b5fb4901599fca69","value":111898327}},"deeffe98ebcb43259bc3a413a70fd114":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_18d5e290aec14797878c9fa7204db755","placeholder":"​","style":"IPY_MODEL_7735c539218e46d99592c99166f64b2a","value":"100%"}},"f5e29fe5fb32474997b9b51da58747c1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_deeffe98ebcb43259bc3a413a70fd114","IPY_MODEL_b668e3fa0c5c4bc5b6a5d8a7252cbb95","IPY_MODEL_a6b3bc121c4f4a4499146e843fc0450b"],"layout":"IPY_MODEL_65ecd141d8084cfbb3e82e6527b156d2"}}}}},"nbformat":4,"nbformat_minor":0}
