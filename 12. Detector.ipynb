{"cells":[{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23944,"status":"ok","timestamp":1719318501090,"user":{"displayName":"PRISCO TROTTA","userId":"07883661950882496888"},"user_tz":-120},"id":"VYAawZxff4dP","outputId":"1ee647b4-a817-4594-d76b-28a6e64f5097"},"outputs":[{"name":"stdout","output_type":"stream","text":["Not running on Google Colab. \n"]}],"source":["try:\n","    from google.colab import drive\n","    IN_COLAB = True\n","    print(\"Running on Google Colab. \")\n","    drive.mount('/content/drive')\n","except:\n","    IN_COLAB = False\n","    print(\"Not running on Google Colab. \")"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cuda\n"]}],"source":["import torch \n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using {device}\")"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"ae0Jm8Ylf4dT"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"uGOA0mLjf4dU"},"outputs":[],"source":["from PIL import Image\n","import os\n","import numpy as np\n","from torch.utils.data import Dataset, DataLoader\n","\n","class VGGFace2Dataset(Dataset):\n","    def __init__(self, root_dir, image_size=(160, 160), transform=None, device='cpu'):\n","        self.root_dir = root_dir\n","        self.image_size = image_size\n","        self.transform = transform\n","        self.device = device\n","        # List of files in the dataset\n","        self.file_list = []\n","        self.img_list = []\n","        for root, dirs, files in os.walk(self.root_dir):\n","            for file in files:\n","                try:\n","                    img = Image.open(os.path.join(root, file)).convert('RGB')\n","                    self.file_list.append(os.path.join(root, file))\n","                    if self.transform:\n","                        img = self.transform(img)\n","                    img = img.to(self.device)\n","                    self.img_list.append(img)\n","                except:\n","                    print(f\"Error loading file {os.path.join(root, file)}\")\n","\n","\n","    def __len__(self):\n","        return len(self.file_list)\n","\n","\n","    def __getitem__(self, idx):\n","        img_path = self.file_list[idx]\n","        # verify if image is not corrupted if is corrupted call __getitem__ again\n","        img = self.img_list[idx]\n","        # Extract the label from the file path\n","        label = os.path.split(os.path.dirname(img_path))[-1]\n","\n","        labels = torch.tensor(int(label)).to(self.device)\n","\n","        return img, labels"]},{"cell_type":"markdown","metadata":{"id":"4uwxPjF3f4dW"},"source":["Dipende poi su quale se NN1 o NN2 va ad essere utilizzato"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37371,"status":"ok","timestamp":1719318549178,"user":{"displayName":"PRISCO TROTTA","userId":"07883661950882496888"},"user_tz":-120},"id":"aYsTOj-Xf4dZ","outputId":"dd712c04-f3ea-4eca-f13a-9a70fc08b31e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Error loading file G:\\Drive condivisi\\AI4CYBSEC\\face_dataset\\train_set_detector_df_cw\\train\\0\\n008229_0565_01.jpg\n","Training Set length: 14396\n","Validation Set length: 3600\n"]}],"source":["import os\n","\n","\n","# Define transforms for training and validation\n","transform = transforms.Compose([\n","    transforms.Resize((160, 160)),\n","    transforms.ToTensor(),\n","])\n","\n","# Set up directories\n","if IN_COLAB:\n","    dataset_dir = '/content/drive/Shareddrives/AI4CYBSEC/face_dataset/dataset_prova_prisco'\n","else:\n","    dataset_dir = 'G:\\Drive condivisi\\AI4CYBSEC\\\\face_dataset\\\\train_set_detector_df_cw'\n","\n","\n","train_set_path = os.path.join(dataset_dir,'train')\n","\n","\n","# Define dataset\n","dataset_train = VGGFace2Dataset(root_dir=train_set_path, transform=transform, device=device)\n","\n","# Check the length of the dataset\n","print(\"Training Set length:\", len(dataset_train))\n","\n","# Create DataLoader\n","batch_size = 32\n","dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n","\n","\n","# Load datasets and Set up data loaders\n","# VAL\n","val_set_path = os.path.join(dataset_dir,'val')\n","\n","# Define dataset\n","dataset_val = VGGFace2Dataset(root_dir=val_set_path, transform=transform, device=device)\n","\n","# Check the length of the dataset\n","print(\"Validation Set length:\", len(dataset_val))\n","\n","# Create DataLoader\n","batch_size = 32\n","dataloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"cD-hmCewzOaE"},"source":["## Alternatica con ResNet"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":892,"status":"ok","timestamp":1719318550051,"user":{"displayName":"PRISCO TROTTA","userId":"07883661950882496888"},"user_tz":-120},"id":"-ePz7mC1zR5s","outputId":"9086c425-5031-4c89-b424-348de097d77a"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Program_for_ML\\envs\\AI4CyberSec\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","c:\\Program_for_ML\\envs\\AI4CyberSec\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import ImageFolder\n","\n","# Definiamo il modello pre-addestrato (es. mobileNetV2)\n","model = models.mobilenet_v2(pretrained=True)\n","\n","# Sostituiamo il classificatore dell'ultimo layer con un nuovo classificatore\n","model.classifier[1] = nn.Linear(model.classifier[1].in_features, 2)\n","# 2 è il numero di classi binarie\n","\n","\n","#  # Sostituire 2 con il numero di classi binarie\n","model = model.to(device)\n","# Definiamo l'ottimizzatore e la loss function\n","#use optimizer AdamW \n","optimizer = optim.AdamW(model.parameters(), lr=0.001)\n","criterion = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"elapsed":5550,"status":"error","timestamp":1719322826200,"user":{"displayName":"PRISCO TROTTA","userId":"07883661950882496888"},"user_tz":-120},"id":"3D63uMbWDLJK","outputId":"c9eae8f1-c418-4a7b-f15e-f6dbcbaf9b9b"},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch 1/100 training: 100%|██████████| 450/450 [00:07<00:00, 62.56it/s]\n","Epoch 1/100 validation: 100%|██████████| 113/113 [00:00<00:00, 230.44it/s]\n"]},{"name":"stdout","output_type":"stream","text":["  Epoch [1/100], Train Loss: 0.5993, Train Accuracy: 0.6517, Validation Accuracy: 0.8089\n","  Best Validation accuracy: 0.8089\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2/100 training: 100%|██████████| 450/450 [00:06<00:00, 65.63it/s]\n","Epoch 2/100 validation: 100%|██████████| 113/113 [00:00<00:00, 234.27it/s]\n"]},{"name":"stdout","output_type":"stream","text":["  Epoch [2/100], Train Loss: 0.1925, Train Accuracy: 0.9190, Validation Accuracy: 0.9567\n","  Best Validation accuracy: 0.9567\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3/100 training: 100%|██████████| 450/450 [00:06<00:00, 67.16it/s]\n","Epoch 3/100 validation: 100%|██████████| 113/113 [00:00<00:00, 235.78it/s]\n"]},{"name":"stdout","output_type":"stream","text":["  Epoch [3/100], Train Loss: 0.1049, Train Accuracy: 0.9558, Validation Accuracy: 0.9628\n","  Best Validation accuracy: 0.9628\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4/100 training: 100%|██████████| 450/450 [00:06<00:00, 67.04it/s]\n","Epoch 4/100 validation: 100%|██████████| 113/113 [00:00<00:00, 228.36it/s]\n"]},{"name":"stdout","output_type":"stream","text":["  Epoch [4/100], Train Loss: 0.0787, Train Accuracy: 0.9670, Validation Accuracy: 0.9769\n","  Best Validation accuracy: 0.9769\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5/100 training: 100%|██████████| 450/450 [00:06<00:00, 64.41it/s]\n","Epoch 5/100 validation: 100%|██████████| 113/113 [00:00<00:00, 230.74it/s]\n"]},{"name":"stdout","output_type":"stream","text":["  Epoch [5/100], Train Loss: 0.0663, Train Accuracy: 0.9722, Validation Accuracy: 0.9744\n","  Best Validation accuracy: 0.9769\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 6/100 training: 100%|██████████| 450/450 [00:06<00:00, 66.06it/s]\n","Epoch 6/100 validation: 100%|██████████| 113/113 [00:00<00:00, 226.91it/s]\n"]},{"name":"stdout","output_type":"stream","text":["  Epoch [6/100], Train Loss: 0.0564, Train Accuracy: 0.9757, Validation Accuracy: 0.9547\n","  Best Validation accuracy: 0.9769\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 7/100 training: 100%|██████████| 450/450 [00:07<00:00, 62.09it/s]\n","Epoch 7/100 validation: 100%|██████████| 113/113 [00:00<00:00, 226.00it/s]\n"]},{"name":"stdout","output_type":"stream","text":["  Epoch [7/100], Train Loss: 0.0500, Train Accuracy: 0.9778, Validation Accuracy: 0.9767\n","  Best Validation accuracy: 0.9769\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 8/100 training: 100%|██████████| 450/450 [00:07<00:00, 63.16it/s]\n","Epoch 8/100 validation: 100%|██████████| 113/113 [00:00<00:00, 211.96it/s]\n"]},{"name":"stdout","output_type":"stream","text":["  Epoch [8/100], Train Loss: 0.0493, Train Accuracy: 0.9795, Validation Accuracy: 0.9769\n","  Best Validation accuracy: 0.9769\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 9/100 training: 100%|██████████| 450/450 [00:06<00:00, 64.94it/s]\n","Epoch 9/100 validation: 100%|██████████| 113/113 [00:00<00:00, 230.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Epoch [9/100], Train Loss: 0.0485, Train Accuracy: 0.9804, Validation Accuracy: 0.9661\n","  Best Validation accuracy: 0.9769\n","Early stopping at epoch 9\n","Best model weights saved\n","Best Validation accuracy:  0.9769444444444444\n","Confusion Matrix:  [[1950   51]\n"," [  32 1567]]\n","Classification Report:                precision    recall  f1-score   support\n","\n","           0       0.98      0.97      0.98      2001\n","           1       0.97      0.98      0.97      1599\n","\n","    accuracy                           0.98      3600\n","   macro avg       0.98      0.98      0.98      3600\n","weighted avg       0.98      0.98      0.98      3600\n","\n","Precision Score:  0.9684796044499382\n","Recall Score:  0.9799874921826142\n","F1 Score:  0.9741995648119366\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["from tqdm import tqdm\n","import torch\n","import os\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n","\n","\n","# Addestramento del classificatore\n","num_epochs = 100\n","patience = 5  # Numero massimo di epoche senza miglioramenti nella validation accuracy\n","no_improvement_count = 0  # Conta il numero di epoche senza miglioramenti\n","\n","best_val_accuracy = 0.0  # Inizializziamo la miglior accuracy sul validation set a 0\n","best_model_weights = None  # Variabile per memorizzare i pesi del miglior modello\n","\n","\n","for epoch in range(num_epochs):\n","    running_loss = 0.0\n","    total_correct = 0\n","    total_samples = 0\n","\n","    model.train()  # Imposta il modello in modalità training\n","\n","    for inputs, labels in tqdm(dataloader_train, desc=f\"Epoch {epoch+1}/{num_epochs} training\"):\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","        _, predicted = torch.max(outputs, 1)\n","        total_samples += labels.size(0)\n","        total_correct += (predicted == labels).sum().item()\n","\n","    train_loss = running_loss / len(dataloader_train)\n","    train_accuracy = total_correct / total_samples\n","\n","    # print(f'  Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n","\n","    # Valutazione sul validation set\n","    model.eval()  # Imposta il modello in modalità valutazione (non addestramento)\n","    all_predictions = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for inputs, labels in tqdm(dataloader_val, desc=f\"Epoch {epoch+1}/{num_epochs} validation\"):\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            _, predicted = torch.max(outputs, 1)\n","            all_predictions.extend(predicted.tolist())\n","            all_labels.extend(labels.tolist())\n","\n","    val_accuracy = accuracy_score(all_labels, all_predictions)\n","\n","\n","    # TP, FP, TN, FN\n","    \n","    # classificazione_report = classification_report(all_labels, all_predictions)\n","\n","    print(f'  Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n","\n","    # Torniamo in modalità training per la prossima epoca\n","    model.train()\n","\n","    # Salviamo i pesi del modello se l'accuracy sul validation set attuale è migliore della precedente\n","    if val_accuracy > best_val_accuracy:\n","        best_val_accuracy = val_accuracy\n","        #save classificatio report\n","        conf_matrix = confusion_matrix(all_labels, all_predictions)\n","        report = classification_report(all_labels, all_predictions)\n","        precision_score_value = precision_score(all_labels, all_predictions)\n","        recall_score_value = recall_score(all_labels, all_predictions)\n","        f1_score_value = f1_score(all_labels, all_predictions)\n","        #save confusion matrix\n","        best_model_weights = model.state_dict()\n","        no_improvement_count = 0  # Resettiamo il contatore di epoche senza miglioramenti\n","    else:\n","        no_improvement_count += 1  # Incrementiamo il contatore di epoche senza miglioramenti\n","    print(f'  Best Validation accuracy: {best_val_accuracy:.4f}')\n","    # Verifica del criterio di early stopping\n","    if no_improvement_count >= patience:\n","        print(f'Early stopping at epoch {epoch+1}')\n","        break\n","\n","# Salviamo i pesi del miglior modello addestrato\n","if IN_COLAB:\n","    directory = \"/content/drive/Shareddrives/AI4CYBSEC/models/\"\n","else:\n","    directory = \"G:\\Drive condivisi\\AI4CYBSEC\\models\"\n","if not os.path.exists(directory):\n","    os.makedirs(directory)\n","\n","if best_model_weights is not None:\n","    torch.save(best_model_weights, os.path.join(directory,'best_mobilenetv2_df_cw.pth'))\n","    print(\"Best model weights saved\")\n","    print(\"Best Validation accuracy: \", best_val_accuracy)\n","    print(\"Confusion Matrix: \", conf_matrix)\n","    print(\"Classification Report: \", report)\n","    print(\"Precision Score: \", precision_score_value)\n","    print(\"Recall Score: \", recall_score_value)\n","    print(\"F1 Score: \", f1_score_value)\n"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"b9PvJYLHg0Cy"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Program_for_ML\\envs\\AI4CyberSec\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","c:\\Program_for_ML\\envs\\AI4CyberSec\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 32, 80, 80]             864\n","       BatchNorm2d-2           [-1, 32, 80, 80]              64\n","             ReLU6-3           [-1, 32, 80, 80]               0\n","            Conv2d-4           [-1, 32, 80, 80]             288\n","       BatchNorm2d-5           [-1, 32, 80, 80]              64\n","             ReLU6-6           [-1, 32, 80, 80]               0\n","            Conv2d-7           [-1, 16, 80, 80]             512\n","       BatchNorm2d-8           [-1, 16, 80, 80]              32\n","  InvertedResidual-9           [-1, 16, 80, 80]               0\n","           Conv2d-10           [-1, 96, 80, 80]           1,536\n","      BatchNorm2d-11           [-1, 96, 80, 80]             192\n","            ReLU6-12           [-1, 96, 80, 80]               0\n","           Conv2d-13           [-1, 96, 40, 40]             864\n","      BatchNorm2d-14           [-1, 96, 40, 40]             192\n","            ReLU6-15           [-1, 96, 40, 40]               0\n","           Conv2d-16           [-1, 24, 40, 40]           2,304\n","      BatchNorm2d-17           [-1, 24, 40, 40]              48\n"," InvertedResidual-18           [-1, 24, 40, 40]               0\n","           Conv2d-19          [-1, 144, 40, 40]           3,456\n","      BatchNorm2d-20          [-1, 144, 40, 40]             288\n","            ReLU6-21          [-1, 144, 40, 40]               0\n","           Conv2d-22          [-1, 144, 40, 40]           1,296\n","      BatchNorm2d-23          [-1, 144, 40, 40]             288\n","            ReLU6-24          [-1, 144, 40, 40]               0\n","           Conv2d-25           [-1, 24, 40, 40]           3,456\n","      BatchNorm2d-26           [-1, 24, 40, 40]              48\n"," InvertedResidual-27           [-1, 24, 40, 40]               0\n","           Conv2d-28          [-1, 144, 40, 40]           3,456\n","      BatchNorm2d-29          [-1, 144, 40, 40]             288\n","            ReLU6-30          [-1, 144, 40, 40]               0\n","           Conv2d-31          [-1, 144, 20, 20]           1,296\n","      BatchNorm2d-32          [-1, 144, 20, 20]             288\n","            ReLU6-33          [-1, 144, 20, 20]               0\n","           Conv2d-34           [-1, 32, 20, 20]           4,608\n","      BatchNorm2d-35           [-1, 32, 20, 20]              64\n"," InvertedResidual-36           [-1, 32, 20, 20]               0\n","           Conv2d-37          [-1, 192, 20, 20]           6,144\n","      BatchNorm2d-38          [-1, 192, 20, 20]             384\n","            ReLU6-39          [-1, 192, 20, 20]               0\n","           Conv2d-40          [-1, 192, 20, 20]           1,728\n","      BatchNorm2d-41          [-1, 192, 20, 20]             384\n","            ReLU6-42          [-1, 192, 20, 20]               0\n","           Conv2d-43           [-1, 32, 20, 20]           6,144\n","      BatchNorm2d-44           [-1, 32, 20, 20]              64\n"," InvertedResidual-45           [-1, 32, 20, 20]               0\n","           Conv2d-46          [-1, 192, 20, 20]           6,144\n","      BatchNorm2d-47          [-1, 192, 20, 20]             384\n","            ReLU6-48          [-1, 192, 20, 20]               0\n","           Conv2d-49          [-1, 192, 20, 20]           1,728\n","      BatchNorm2d-50          [-1, 192, 20, 20]             384\n","            ReLU6-51          [-1, 192, 20, 20]               0\n","           Conv2d-52           [-1, 32, 20, 20]           6,144\n","      BatchNorm2d-53           [-1, 32, 20, 20]              64\n"," InvertedResidual-54           [-1, 32, 20, 20]               0\n","           Conv2d-55          [-1, 192, 20, 20]           6,144\n","      BatchNorm2d-56          [-1, 192, 20, 20]             384\n","            ReLU6-57          [-1, 192, 20, 20]               0\n","           Conv2d-58          [-1, 192, 10, 10]           1,728\n","      BatchNorm2d-59          [-1, 192, 10, 10]             384\n","            ReLU6-60          [-1, 192, 10, 10]               0\n","           Conv2d-61           [-1, 64, 10, 10]          12,288\n","      BatchNorm2d-62           [-1, 64, 10, 10]             128\n"," InvertedResidual-63           [-1, 64, 10, 10]               0\n","           Conv2d-64          [-1, 384, 10, 10]          24,576\n","      BatchNorm2d-65          [-1, 384, 10, 10]             768\n","            ReLU6-66          [-1, 384, 10, 10]               0\n","           Conv2d-67          [-1, 384, 10, 10]           3,456\n","      BatchNorm2d-68          [-1, 384, 10, 10]             768\n","            ReLU6-69          [-1, 384, 10, 10]               0\n","           Conv2d-70           [-1, 64, 10, 10]          24,576\n","      BatchNorm2d-71           [-1, 64, 10, 10]             128\n"," InvertedResidual-72           [-1, 64, 10, 10]               0\n","           Conv2d-73          [-1, 384, 10, 10]          24,576\n","      BatchNorm2d-74          [-1, 384, 10, 10]             768\n","            ReLU6-75          [-1, 384, 10, 10]               0\n","           Conv2d-76          [-1, 384, 10, 10]           3,456\n","      BatchNorm2d-77          [-1, 384, 10, 10]             768\n","            ReLU6-78          [-1, 384, 10, 10]               0\n","           Conv2d-79           [-1, 64, 10, 10]          24,576\n","      BatchNorm2d-80           [-1, 64, 10, 10]             128\n"," InvertedResidual-81           [-1, 64, 10, 10]               0\n","           Conv2d-82          [-1, 384, 10, 10]          24,576\n","      BatchNorm2d-83          [-1, 384, 10, 10]             768\n","            ReLU6-84          [-1, 384, 10, 10]               0\n","           Conv2d-85          [-1, 384, 10, 10]           3,456\n","      BatchNorm2d-86          [-1, 384, 10, 10]             768\n","            ReLU6-87          [-1, 384, 10, 10]               0\n","           Conv2d-88           [-1, 64, 10, 10]          24,576\n","      BatchNorm2d-89           [-1, 64, 10, 10]             128\n"," InvertedResidual-90           [-1, 64, 10, 10]               0\n","           Conv2d-91          [-1, 384, 10, 10]          24,576\n","      BatchNorm2d-92          [-1, 384, 10, 10]             768\n","            ReLU6-93          [-1, 384, 10, 10]               0\n","           Conv2d-94          [-1, 384, 10, 10]           3,456\n","      BatchNorm2d-95          [-1, 384, 10, 10]             768\n","            ReLU6-96          [-1, 384, 10, 10]               0\n","           Conv2d-97           [-1, 96, 10, 10]          36,864\n","      BatchNorm2d-98           [-1, 96, 10, 10]             192\n"," InvertedResidual-99           [-1, 96, 10, 10]               0\n","          Conv2d-100          [-1, 576, 10, 10]          55,296\n","     BatchNorm2d-101          [-1, 576, 10, 10]           1,152\n","           ReLU6-102          [-1, 576, 10, 10]               0\n","          Conv2d-103          [-1, 576, 10, 10]           5,184\n","     BatchNorm2d-104          [-1, 576, 10, 10]           1,152\n","           ReLU6-105          [-1, 576, 10, 10]               0\n","          Conv2d-106           [-1, 96, 10, 10]          55,296\n","     BatchNorm2d-107           [-1, 96, 10, 10]             192\n","InvertedResidual-108           [-1, 96, 10, 10]               0\n","          Conv2d-109          [-1, 576, 10, 10]          55,296\n","     BatchNorm2d-110          [-1, 576, 10, 10]           1,152\n","           ReLU6-111          [-1, 576, 10, 10]               0\n","          Conv2d-112          [-1, 576, 10, 10]           5,184\n","     BatchNorm2d-113          [-1, 576, 10, 10]           1,152\n","           ReLU6-114          [-1, 576, 10, 10]               0\n","          Conv2d-115           [-1, 96, 10, 10]          55,296\n","     BatchNorm2d-116           [-1, 96, 10, 10]             192\n","InvertedResidual-117           [-1, 96, 10, 10]               0\n","          Conv2d-118          [-1, 576, 10, 10]          55,296\n","     BatchNorm2d-119          [-1, 576, 10, 10]           1,152\n","           ReLU6-120          [-1, 576, 10, 10]               0\n","          Conv2d-121            [-1, 576, 5, 5]           5,184\n","     BatchNorm2d-122            [-1, 576, 5, 5]           1,152\n","           ReLU6-123            [-1, 576, 5, 5]               0\n","          Conv2d-124            [-1, 160, 5, 5]          92,160\n","     BatchNorm2d-125            [-1, 160, 5, 5]             320\n","InvertedResidual-126            [-1, 160, 5, 5]               0\n","          Conv2d-127            [-1, 960, 5, 5]         153,600\n","     BatchNorm2d-128            [-1, 960, 5, 5]           1,920\n","           ReLU6-129            [-1, 960, 5, 5]               0\n","          Conv2d-130            [-1, 960, 5, 5]           8,640\n","     BatchNorm2d-131            [-1, 960, 5, 5]           1,920\n","           ReLU6-132            [-1, 960, 5, 5]               0\n","          Conv2d-133            [-1, 160, 5, 5]         153,600\n","     BatchNorm2d-134            [-1, 160, 5, 5]             320\n","InvertedResidual-135            [-1, 160, 5, 5]               0\n","          Conv2d-136            [-1, 960, 5, 5]         153,600\n","     BatchNorm2d-137            [-1, 960, 5, 5]           1,920\n","           ReLU6-138            [-1, 960, 5, 5]               0\n","          Conv2d-139            [-1, 960, 5, 5]           8,640\n","     BatchNorm2d-140            [-1, 960, 5, 5]           1,920\n","           ReLU6-141            [-1, 960, 5, 5]               0\n","          Conv2d-142            [-1, 160, 5, 5]         153,600\n","     BatchNorm2d-143            [-1, 160, 5, 5]             320\n","InvertedResidual-144            [-1, 160, 5, 5]               0\n","          Conv2d-145            [-1, 960, 5, 5]         153,600\n","     BatchNorm2d-146            [-1, 960, 5, 5]           1,920\n","           ReLU6-147            [-1, 960, 5, 5]               0\n","          Conv2d-148            [-1, 960, 5, 5]           8,640\n","     BatchNorm2d-149            [-1, 960, 5, 5]           1,920\n","           ReLU6-150            [-1, 960, 5, 5]               0\n","          Conv2d-151            [-1, 320, 5, 5]         307,200\n","     BatchNorm2d-152            [-1, 320, 5, 5]             640\n","InvertedResidual-153            [-1, 320, 5, 5]               0\n","          Conv2d-154           [-1, 1280, 5, 5]         409,600\n","     BatchNorm2d-155           [-1, 1280, 5, 5]           2,560\n","           ReLU6-156           [-1, 1280, 5, 5]               0\n","         Dropout-157                 [-1, 1280]               0\n","          Linear-158                    [-1, 2]           2,562\n","================================================================\n","Total params: 2,226,434\n","Trainable params: 2,226,434\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.29\n","Forward/backward pass size (MB): 77.99\n","Params size (MB): 8.49\n","Estimated Total Size (MB): 86.78\n","----------------------------------------------------------------\n"]}],"source":["\n","\n","def load_model(model, model_path):\n","    model.load_state_dict(torch.load(model_path))\n","    model.eval()\n","    return model\n","\n","# Definisci il modello mobilenet_v2\n","model = models.mobilenet_v2(pretrained=True)\n","\n","# Sostituisci il classificatore dell'ultimo layer con un nuovo classificatore\n","model.classifier[1] = nn.Linear(model.classifier[1].in_features, 2)\n","\n","model = model.to(device)\n","\n","\n","# carica i pesi del modello addestrato\n","model = load_model(model, os.path.join(directory,'best_mobilenetv2_df_cw.pth'))\n","\n","import torchsummary\n","\n","# Stampa un riassunto del modello\n","torchsummary.summary(model, (3, 160, 160))\n","\n","def make_inference(model, img_tensor):\n","    #img must be a tensor with shape (N, C, H, W)\n","    model.eval()\n","    with torch.no_grad():\n","        outputs = model(img_tensor)\n","        _, predicted = torch.max(outputs, 1)\n","\n","    return predicted\n","\n","\n","\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
