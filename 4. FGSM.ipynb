{"cells":[{"cell_type":"markdown","metadata":{"id":"YbJM10vaR1Br"},"source":["## 1. Loading prereqs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yG53mUcVs3KP"},"outputs":[],"source":["try:\n","    from google.colab import drive\n","    IN_COLAB = True\n","    print(\"Running on Google Colab. \")\n","except:\n","    IN_COLAB = False\n","    print(\"Not running on Google Colab. \")"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"HQ-NGoOGNt2I"},"outputs":[],"source":["if IN_COLAB:\n","    !pip install facenet-pytorch  # fornisce modelli pre-addestrati PyTorch per compiti di riconoscimento facciale\n","    !pip install Pillow # aggiunge il supporto per l'apertura, la manipolazione e il salvataggio di molti diversi formati di file immagine."]},{"cell_type":"markdown","metadata":{"id":"4OYXJ4qSQix8"},"source":["### Connect to Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CQ1e5FKXQlTh"},"outputs":[],"source":["if IN_COLAB:\n","    from google.colab import drive\n","    drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"hSU0qGu7Z5cr"},"source":["## 2. Load NN1"]},{"cell_type":"markdown","metadata":{"id":"f1ycl1LPTqsz"},"source":["### Load pre-trained model\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IXc7QMPuSeco"},"outputs":[],"source":["# utilizzo la libreria facenet_pytorch per caricare il modello InceptionResnetV1 preaddestrato sul dataset VGGFace2 e abilitare la classificazione.\n","from facenet_pytorch import InceptionResnetV1, MTCNN\n","import torch\n","\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","device = 'cpu'\n","print('Running on device: {}'.format(device))\n","\n","resnet = InceptionResnetV1(pretrained='vggface2', device=device).eval()\n","resnet.classify = True\n","resnet = resnet.to(device)"]},{"cell_type":"markdown","metadata":{"id":"GzhAQvqdS9Ko"},"source":["#### Loading labels of model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tc_BifakDUrs"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","\n","# Il modello è addestrato sulle seguenti Labels:\n","# Carico le labels del dataset VGGFACE\n","fpath = tf.keras.utils.get_file('rcmalli_vggface_labels_v2.npy',\n","                             \"https://github.com/rcmalli/keras-vggface/releases/download/v2.0/rcmalli_vggface_labels_v2.npy\",\n","                             cache_subdir=\"./\")\n","LABELS = np.load(fpath) # List of name\n","\n","for i in range(len(LABELS)):\n","  LABELS[i] = LABELS[i].strip().replace(' ', '').replace('\"', '')"]},{"cell_type":"markdown","metadata":{"id":"M-054Sn2e9ED"},"source":["## 2. Load NN2\n"]},{"cell_type":"markdown","metadata":{"id":"n-sIZrIxfEf8"},"source":["#### Load repository for NN2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"62hKw2aqfFAy"},"outputs":[],"source":["# se non è presente la cartella VGGFACE2_pytorch  clone il repository\n","import os\n","if not os.path.exists('VGGFace2_pytorch'):\n","    !git clone https://github.com/cydonia999/VGGFace2-pytorch.git\n","    !mv VGGFace2-pytorch VGGFACE2_pytorch\n"]},{"cell_type":"markdown","metadata":{"id":"6sKD86nifFe7"},"source":["### Import from repository and import operation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3QXadl-YuMwe"},"outputs":[],"source":["%cd drive/Shareddrives/AI4CYBSEC/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J1DoEniBfF1E"},"outputs":[],"source":["import torch\n","from VGGFace2_pytorch.models import senet as SENet\n","from VGGFace2_pytorch.models.resnet import resnet50 as ResNet\n","from VGGFace2_pytorch import utils\n","from VGGFace2_pytorch.trainer import Validator\n","from torch.utils.data import DataLoader\n","from VGGFace2_pytorch.datasets.vgg_face2 import VGG_Faces2\n","import os\n","from torch.nn.modules.loss import CrossEntropyLoss"]},{"cell_type":"markdown","metadata":{"id":"Xo_66PBOfT24"},"source":["### Load Model from pickel file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N89ZtrRjfUHD"},"outputs":[],"source":["model = SENet.senet50(num_classes = 8631, include_top = True)\n","weights_pickel = \"./in_progress/senet50_ft_weight.pkl\"\n","utils.load_state_dict(model, weights_pickel)\n","device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n","model.to(device)\n","import torchsummary\n","torchsummary.summary(model, (3, 224, 224))"]},{"cell_type":"markdown","metadata":{"id":"Cepg-ZkuZgbH"},"source":["## 3. Load Test Set"]},{"cell_type":"markdown","metadata":{"id":"L-NwB2gKZDLX"},"source":["#### Load the label of Test Set (for NN1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CaAhwqLCXx4Q"},"outputs":[],"source":["# set the path for the dataset\n","if IN_COLAB:\n","    path_dataset = \"/content/drive/Shareddrives/AI4CYBSEC/face_dataset\"\n","else:\n","    path_dataset = \"./face_dataset\"\n","identity_meta_NN1_name = \"meta_identity_NN1.csv\"\n","\n","import pandas as pd\n","import os\n","\n","path_identity_csv =os.path.join(path_dataset,identity_meta_NN1_name)\n","identity_meta_NN1 = pd.read_csv(path_identity_csv)"]},{"cell_type":"markdown","metadata":{"id":"2H276KCNaKsJ"},"source":["## 4. Mapping different label encoding"]},{"cell_type":"markdown","metadata":{"id":"RxfKIdLOg4BR"},"source":["### Mapping label for NN1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"evxVsnB6aPTG"},"outputs":[],"source":["# I want a dictonary related to the label of the Test Set that map the name of celebrities with label associated\n","name_to_id = {}\n","id_to_name = {}\n","for index, row in identity_meta_NN1.iterrows():\n","    # Ora puoi accedere ai valori di ogni riga come segue:\n","    class_id = row['Class_ID']\n","    name = row['Name']\n","    name_to_id[name]=class_id\n","    id_to_name[class_id] = name"]},{"cell_type":"markdown","metadata":{"id":"4YwkBSZsg8ZF"},"source":["### Mapping label for NN2"]},{"cell_type":"markdown","metadata":{"id":"X08pLzwrhAl0"},"source":["#### Function to create image list file for NN2 evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aeas1r7ohFOg"},"outputs":[],"source":["import os\n","def create_image_list_file(root_dir, output_file, ext = '.jpg'):\n","\n","    image_paths = []\n","\n","    for class_id in os.listdir(root_dir):\n","        class_dir = os.path.join(root_dir, class_id)\n","\n","        if os.path.isdir(class_dir):\n","\n","            for filename in os.listdir(class_dir):\n","\n","                if filename.endswith(ext):\n","                    image_path = f\"{os.path.basename(root_dir)}/{class_id}/{filename}\"\n","                    image_paths.append(image_path)\n","\n","    with open(output_file, 'w') as f:\n","        for image_path in image_paths:\n","            f.write(image_path + '\\n')\n","\n","    print(f\"File di output creato con successo: {output_file}\")"]},{"cell_type":"markdown","metadata":{"id":"kjYNGvjGhHQg"},"source":["#### Mapping pipeline NN2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jNNQsWQ6hKeb"},"outputs":[],"source":["test_set_NN2 = \"test_set_MTCNN_NN2\"\n","\n","if IN_COLAB:\n","    output_file = '/content/drive/Shareddrives/AI4CYBSEC/image_list_file_NN2.txt'\n","    meta_file = \"/content/drive/Shareddrives/AI4CYBSEC/face_dataset/identity_meta.csv\"\n","else:\n","    output_file = '/./image_list_file_NN2.txt'\n","    meta_file = \"./face_dataset/identity_meta.csv\"\n","\n","root_dir = os.path.join(path_dataset,test_set_NN2)\n","create_image_list_file(root_dir, output_file)\n","id_label_dict = utils.get_id_label_map(meta_file)"]},{"cell_type":"markdown","metadata":{"id":"o5aQtJxlwUE4"},"source":["## 5. Dataset Class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YCGxCH5YqSQQ"},"outputs":[],"source":["from PIL import Image\n","import os\n","import torch\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader\n","\n","class VGGFace2Dataset(Dataset):\n","    def __init__(self, root_dir, image_size=(160, 160), transform=None):\n","        self.root_dir = root_dir\n","        self.image_size = image_size\n","        self.transform = transform\n","\n","        # List of files in the dataset\n","        self.file_list = []\n","        for root, dirs, files in os.walk(self.root_dir):\n","            for file in files:\n","                self.file_list.append(os.path.join(root, file))\n","\n","    def __len__(self):\n","        return len(self.file_list)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.file_list[idx]\n","        img = Image.open(img_path).resize(self.image_size)\n","\n","        # Extract the label from the file path\n","        label = os.path.split(os.path.dirname(img_path))[-1]\n","\n","        if self.transform:\n","            img = self.transform(img)\n","\n","        return img, label"]},{"cell_type":"markdown","metadata":{"id":"m33yvIJoSUkr"},"source":["## 6. Evaluate model NN1\n"]},{"cell_type":"markdown","metadata":{"id":"foZQE3LkdGCj"},"source":["### Utility Functions for NN1 with mapping labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5gjQcj8Ij6gr"},"outputs":[],"source":["from PIL import Image\n","from torchvision import transforms\n","import torch\n","import numpy as np\n","from matplotlib import pyplot as plt\n","from tqdm import tqdm\n","\n","\n","def make_inference(model, image_tensors, name_to_id, device):\n","    \"\"\"\n","    Takes input image tensor and returns the label associated with the network's prediction.\n","\n","    \"\"\"\n","    model.to(device)\n","    model.eval()\n","    # Move image tensors to the specified device\n","    image_tensors = image_tensors.to(device)\n","\n","    probs = model(image_tensors)\n","    #print(\"probs\", probs)\n","\n","    # Get the number of elements along the first dimension\n","    num_elements = probs.size(0)\n","\n","    # Initialize two lists to store the argmax\n","    argmax_list_1 = []\n","    argmax_list_2 = []\n","\n","    # Compute argmax for each element along the first dimension\n","    for i in range(num_elements):\n","        #target_class = np.array(probs[i].detach().cpu().numpy()).argmax()  # Move to CPU for numpy operations\n","        target_class = torch.argmax(probs[i]).item()\n","        argmax_list_1.append(name_to_id[LABELS[target_class]])\n","        argmax_list_2.append(target_class)\n","\n","    return argmax_list_1, argmax_list_2\n","\n","\n","def validate(dataset, model, name_to_id, device):\n","    \"\"\"\n","    Validates a model on a dataset and returns the accuracy.\n","\n","    Args:\n","        dataset: Dataloader to validate the model on.\n","        model: Model to validate.\n","        device: Device to perform inference on.\n","\n","    Returns:\n","        accuracy: Accuracy of the model on the dataset.\n","    \"\"\"\n","    model.eval()\n","    correct_predictions = 0\n","    total_samples = len(dataset) * dataset.batch_size\n","\n","    with torch.no_grad():  # Disable gradient calculation\n","        for images, labels in tqdm(dataset, desc=\"Validating model\"):\n","            #images = mtcnn(images)\n","            predicted_classes, _ = make_inference(model, images, name_to_id, device)\n","            correct_predictions += sum(pred == label for pred, label in zip(predicted_classes, labels))\n","\n","    # Compute accuracy\n","    accuracy = correct_predictions / total_samples\n","    return accuracy\n","\n","\n","def plot_image(image, label):\n","  \"\"\"\n","  prende in ingresso le PIL.Image del campione originale e del corrispondete adversarial sample e li plotta\n","  \"\"\"\n","  plt.figure()\n","  plt.matshow(image)\n","  plt.title(\"Model Prediction: {}\".format(label))\n","  plt.show()"]},{"cell_type":"markdown","metadata":{"id":"1e3M3Ojj2h4R"},"source":["### Validation on Clean Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KZF6hf0V2gZO"},"outputs":[],"source":["# Create transform for image resizing and normalization\n","data_transform = transforms.Compose([\n","    transforms.Resize((160, 160)),\n","    transforms.ToTensor()\n","])\n","\n","if IN_COLAB:\n","    test_set_path = \"/content/drive/Shareddrives/AI4CYBSEC/face_dataset/test_set_MTCNN\"\n","else:\n","\n","    test_set_path = \"./face_dataset/test_set_MTCNN\"\n","# Define dataset\n","dataset = VGGFace2Dataset(root_dir=test_set_path, transform=data_transform)\n","dataset_len = len(dataset)\n","\n","# Check the length of the dataset\n","print(\"Dataset length:\", dataset_len)\n","\n","# Create DataLoader\n","batch_size = 1\n","dataloader_NN1 = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n","\n","# accuracy_clean_data = validate(dataloader, resnet, name_to_id, device)\n","accuracy_clean_data = 0.832\n","print(\"\\nAccuracy on clean data: \" + str(round(accuracy_clean_data, 3)))"]},{"cell_type":"markdown","metadata":{"id":"yZkxci0YhwWM"},"source":["## 7. Evaluate Model NN2"]},{"cell_type":"markdown","metadata":{"id":"qxksRj1nh1nn"},"source":["### Utility functions for NN2\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-zi2jnvFh5HO"},"outputs":[],"source":["def preprocessing_on_tensor(img_tensor, mean_bgr=np.array([91.4953, 103.8827, 131.0912])):\n","    \"\"\"\n","    Perform preprocessing on the input image tensor for the model.\n","\n","    :param img_tensor: immagine with shape (C, H, W) and values in [0, 1]\n","    :return: immagine normalizzata using mean_bgr with shape (1, C, H, W)\n","\n","    \"\"\"\n","    img = img_tensor.squeeze(0)\n","    img = img.numpy()\n","    img = (img * 255).astype(np.uint8)\n","    mean_bgr = np.array([91.4953,103.8827, 131.0912])\n","    # img è C x H x W --> H x W x C\n","    img = np.transpose(img, (1, 2, 0))\n","    img = img[:, :, ::-1]  # RGB -> BGR\n","    img = img.astype(np.float32)\n","    img -= mean_bgr\n","    img = img.transpose(2, 0, 1)\n","    img_tensor = torch.from_numpy(img).unsqueeze(0)\n","    return img_tensor\n","\n","\n","def make_inference_NN2(model, img_tensor, device, with_preprocessing=True):\n","    \"\"\"\n","    Esegue l'inferenza su un'immagine.\n","    :param model: modello\n","    :param img_tensor: immagine trasformata\n","    :param device: dispositivo\n","    :return: predizione\n","    \"\"\"\n","    if with_preprocessing:\n","        img_tensor = img_tensor.squeeze(0)\n","        img_tensor = preprocessing_on_tensor(img_tensor)\n","    model.eval()\n","    img_tensor = img_tensor.to(device)\n","\n","    with torch.no_grad():\n","        output = model(img_tensor)\n","        pred = torch.argmax(output, dim=1).item()\n","    return pred"]},{"cell_type":"markdown","metadata":{"id":"6k0BkkKLoVhp"},"source":["## 8. FGSM attack"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wdvkvH_PofkK"},"outputs":[],"source":["if IN_COLAB:\n","    !pip install adversarial-robustness-toolbox[all]\n","import art"]},{"cell_type":"markdown","metadata":{"id":"f_IfYmilGMS5"},"source":["### Utility Functions to perform attacks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3vO38S_w6zgF"},"outputs":[],"source":["### Utility Functions to perform attacks\n","import numpy as np\n","import csv\n","from datetime import datetime\n","import json\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","\n","def compute_perturbation(original_images, adversarial_samples):\n","    perturbations = []\n","    for original_image, adversarial_sample in zip(original_images, adversarial_samples):\n","        perturbation = np.mean(np.abs((np.array(original_image) - np.array(adversarial_sample))))\n","        perturbations.append(perturbation)\n","    return round(float(np.mean(perturbations)),3)\n","\n","\n","def sec_curve(strength_values, accuracies_values, constant_values, strength_name, target_class=None,\n","              attack=None, avg_perturbations=None, accuracy_on_target_class=None, network = \"NN1\"):\n","    fig, ax = plt.subplots()\n","    # Costruisci la stringa per i valori costanti\n","    constant_str = ', '.join([f'{key}: {value}' for key, value in constant_values.items()])\n","    line = ax.plot(np.array(strength_values), np.array(accuracies_values), 'b--', label=f'{network} - {constant_str}')\n","\n","\n","    # Aggiungi i valori costanti come parte della legenda\n","    if target_class:\n","        plt.title('Security Curve for Target Class {}'.format(target_class))\n","    else:\n","        plt.title('Security Curve')\n","\n","    # Aggiungi l'attacco al titolo\n","    if attack:\n","        plt.title(f'{attack} - {plt.gca().get_title()}')\n","\n","    plt.xlabel('Attack strength ({})'.format(strength_name))\n","    plt.ylabel('Accuracy Test')\n","    plt.grid()\n","\n","    # Aggiungi il diagramma a barre di colore arancione per avg_perturbations\n","    if avg_perturbations:\n","        x = np.array(strength_values)\n","        ax2 = ax.twinx()\n","        bar = ax2.bar(x, avg_perturbations, color='orange', alpha=0.5, width=0.01, label='Avg Perturbations')\n","        ax2.set_ylabel('Avg Perturbations')\n","\n","\n","\n","    if accuracy_on_target_class:\n","        # Aggiungi la curva di accuratezza per la classe target\n","        ax.plot(np.array(strength_values), np.array(accuracy_on_target_class), 'r--', label=f'{network} - mis_targ/miss')\n","\n","    # Unisci le linee e le barre in una lista per la legenda\n","    handles, labels = ax.get_legend_handles_labels()\n","    if avg_perturbations:\n","        handles2, labels2 = ax2.get_legend_handles_labels()\n","        handles += handles2\n","        labels += labels2\n","\n","    # Mostra la legenda\n","    #plt.legend(handles, labels, loc='upper right', shadow=True, fontsize='small')\n","    #voglio che la legenda sia fuori dal grafico\n","    plt.legend(handles, labels, loc='upper left', bbox_to_anchor=(1.2, 1), shadow=True, fontsize='small')\n","    plt.show()\n","\n","\n","def save_to_csv(attack_name, targeted, target_class, strength_name, strength_values, accuracy_values, constant_values, avg_perturbations, file_path, accuracy_on_target_class=None):\n","    # Intestazione del file CSV\n","    header = [\"timestamp\", \"attacco\", \"targeted\", \"target_class\", \"strength_name\", \"strength_values\", \"accuracy_values\", \"constant_values\", \"avg_perturbations\", \"accuracy_on_target_class\"]\n","\n","    # Creazione della tupla con i valori da scrivere nel CSV\n","    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","    # Converti le liste di valori in stringhe JSON\n","    strength_values_json = json.dumps(strength_values)\n","    accuracy_values_json = json.dumps(accuracy_values)\n","    avg_perturbations_json = json.dumps(avg_perturbations)  # Converti avg_perturbations in una stringa JSON\n","    # Modifica: formatta strength_values come una lista di numeri invece di una stringa JSON\n","    accuracy_values_list = [float(val) for val in accuracy_values]\n","    # Modifica: converti accuracy_values in una lista di numeri\n","    row = (timestamp, attack_name, targeted, target_class, strength_name, strength_values_json, accuracy_values_list, json.dumps(constant_values), avg_perturbations_json)\n","\n","    # Se accuracy_on_target_class è fornito e non è None, includilo nella tupla\n","    if accuracy_on_target_class is not None:\n","        accuracy_on_target_class_json = json.dumps(accuracy_on_target_class)  # Converti accuracy_on_target_class in una stringa JSON\n","        row += (accuracy_on_target_class_json,)\n","    else:\n","        row += (None,)  # Aggiungi None alla tupla\n","\n","    # Scrittura nel file CSV in modalità append ('a')\n","    with open(file_path, mode='a', newline='') as file:\n","        writer = csv.writer(file)\n","        if file.tell() == 0:  # Se il file è vuoto, scrivi l'intestazione\n","            writer.writerow(header)\n","        writer.writerow(row)\n","\n","\n","def read_csv_and_plot(csv_file_path, network=\"NN1\"):\n","    with open(csv_file_path, mode='r') as file:\n","        reader = csv.DictReader(file)\n","\n","        for row in reader:\n","            timestamp = row[\"timestamp\"]\n","            attack_name = row[\"attacco\"]\n","            targeted = row[\"targeted\"]\n","            target_class = row[\"target_class\"]\n","            strength_name = row[\"strength_name\"]\n","            strength_values = json.loads(row[\"strength_values\"])\n","            accuracy_values = json.loads(row[\"accuracy_values\"])\n","            constant_values = json.loads(row[\"constant_values\"])\n","            avg_perturbations = json.loads(row[\"avg_perturbations\"])\n","            accuracy_on_target_class = json.loads(row[\"accuracy_on_target_class\"]) if row[\"accuracy_on_target_class\"] else None\n","\n","            # Controlla se accuracy_values è una stringa JSON e la elabora correttamente\n","            if isinstance(accuracy_values, str):\n","                accuracy_values = json.loads(accuracy_values)\n","\n","            # Controlla se strength_values è una stringa JSON e la elabora correttamente\n","            if isinstance(strength_values, str):\n","                strength_values = json.loads(strength_values)\n","\n","            # Controlla se strength_values è una stringa JSON e la elabora correttamente\n","            if isinstance(avg_perturbations, str):\n","                avg_perturbations = json.loads(avg_perturbations)\n","\n","            # Chiama la funzione sec_curve per plottare la curva\n","            sec_curve(strength_values, accuracy_values, constant_values, strength_name, target_class, attack_name, avg_perturbations, accuracy_on_target_class, network)"]},{"cell_type":"markdown","metadata":{"id":"cnyxNICpGtVq"},"source":["### Initialization of FGSM attack"]},{"cell_type":"markdown","metadata":{"id":"_SZ-NzAkGfu-"},"source":["### Prepare dataset for attacks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dCsLOo031bc8"},"outputs":[],"source":["from art.estimators.classification import PyTorchClassifier\n","from art.attacks.evasion import FastGradientMethod\n","\n","from torch import nn, optim\n","import torchvision.transforms as transforms\n","\n","import numpy as np\n","\n","input_shape = (3,160,160)\n","nb_classes = 8631\n","loss = nn.CrossEntropyLoss()    # Triplett loss = nn.TripletMarginLoss()\n","optimizer = optim.Adam(resnet.parameters())\n","classifier = PyTorchClassifier(model=resnet, loss=loss, input_shape=input_shape, nb_classes=nb_classes, optimizer=optimizer, clip_values=(0, 1))\n","\n","attack_name = \"FGSM\"\n","results_csv = \"./attack_results.csv\"\n","PERFORM_ATTACK_NN1 = False\n","network = 'NN1'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PDIydEhbGkdJ"},"outputs":[],"source":["if PERFORM_ATTACK_NN1:\n","  images_list = []\n","  labels_list = []\n","  for image, label in tqdm(dataloader_NN1, desc=\"Preparing dataset\"):\n","      images_list.append(image.numpy())\n","      labels_list.append(label)"]},{"cell_type":"markdown","metadata":{"id":"IevpN52AUNtF"},"source":["### Perform untargeted FGSM attacks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fF7lKKfshmr1"},"outputs":[],"source":["if PERFORM_ATTACK_NN1:\n","  from tqdm import tqdm\n","\n","  epsilon = 0.1\n","  targeted = False\n","  fgsm_untargeted_attacker = FastGradientMethod(estimator=classifier, eps=epsilon, targeted=targeted)\n","\n","  eps_range = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n","  accuracy_values_untargeted = [accuracy_clean_data]\n","  avg_perturbations = [0]\n","\n","  for eps in tqdm(eps_range, desc='Generating {} attacks'.format(attack_name)):\n","      fgsm_untargeted_attacker.set_params(**{'eps': eps})\n","      nb_correct_eps = 0\n","      images_adv_list = []\n","      for img, label in zip(images_list, labels_list):\n","          x_test_adv = fgsm_untargeted_attacker.generate(img)\n","          images_adv_list.append(x_test_adv)\n","          x_test_adv = torch.tensor(x_test_adv)\n","          x_test_adv_pred,_ = make_inference(resnet, x_test_adv, name_to_id, device)\n","          if x_test_adv_pred[0] == label[0]:\n","              nb_correct_eps += 1\n","          break\n","      accuracy_values_untargeted.append(nb_correct_eps/dataset_len)\n","      avg_perturbations.append(compute_perturbation(images_list, images_adv_list))\n","\n","  eps_range = [0] + eps_range\n","  constant_values = {}\n","  strength_name='eps'\n","  target_class = None\n","  accuracy_on_target_class = None\n","\n","  sec_curve(eps_range, accuracy_values_untargeted, constant_values, strength_name, target_class, attack_name, avg_perturbations, accuracy_on_target_class, network)\n","\n","  # save results on csv\n","  save_to_csv(attack_name, targeted, target_class, strength_name, eps_range, accuracy_values_untargeted, constant_values, avg_perturbations, results_csv)"]},{"cell_type":"markdown","metadata":{"id":"uIIP0xRFN_3E"},"source":["### Perform targeted FGSM attacks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UEd5QQ6aUmUH"},"outputs":[],"source":["if PERFORM_ATTACK_NN1:\n","  from tqdm import tqdm\n","\n","  epsilon = 0.1\n","  targeted=True\n","  fgsm_targeted_attacker = FastGradientMethod(estimator=classifier, eps=epsilon, targeted=targeted)\n","  target_class = 0\n","\n","  eps_range = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n","  accuracy_values_targeted = [accuracy_clean_data]\n","  accuracy_on_target_class = [0]\n","  avg_perturbations = [0]\n","\n","  targeted_label = np.array(target_class)\n","  one_hot_bob_targeted_label = np.expand_dims(tf.keras.utils.to_categorical(targeted_label, num_classes=nb_classes), axis=0)\n","\n","  for eps in tqdm(eps_range, desc='Generating {} attacks'.format(attack_name)):\n","      fgsm_targeted_attacker.set_params(**{'eps': eps})\n","      nb_correct_eps = 0\n","      nb_target_eps = 0\n","      nb_misclassifications = 0\n","      images_adv_list = []\n","      for img, label in zip(images_list, labels_list):\n","          x_test_adv = fgsm_targeted_attacker.generate(img, one_hot_bob_targeted_label)\n","          images_adv_list.append(x_test_adv)\n","          x_test_adv = torch.tensor(x_test_adv)\n","          x_test_adv_pred,x_test_adv_pred_model = make_inference(resnet, x_test_adv, name_to_id, device)\n","          if x_test_adv_pred[0] == label[0]:\n","              nb_correct_eps += 1\n","          else:\n","            nb_misclassifications += 1\n","            if x_test_adv_pred_model[0] == target_class:\n","              nb_target_eps += 1\n","\n","      accuracy_values_targeted.append(nb_correct_eps/dataset_len)\n","      accuracy_on_target_class.append(round(nb_target_eps/nb_misclassifications,3))\n","      avg_perturbations.append(compute_perturbation(images_list, images_adv_list))\n","\n","  eps_range = [0] + eps_range\n","  constant_values = {}\n","  strength_name='eps'\n","  sec_curve(eps_range, accuracy_values_targeted, constant_values, strength_name, target_class, attack_name, avg_perturbations, accuracy_on_target_class, network)\n","\n","  # save results on csv\n","  save_to_csv(attack_name, targeted, target_class, strength_name, eps_range, accuracy_values_targeted, constant_values, avg_perturbations, results_csv, accuracy_on_target_class)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YCqGHL2A_ZBw"},"outputs":[],"source":["if IN_COLAB:\n","    results_csv = \"/content/drive/Shareddrives/AI4CYBSEC/results/FGSM/attack_results_NN1.csv\"\n","else:\n","    results_csv = \"./results/FGSM/attack_results_NN1.csv\"\n","network = 'NN1'\n","read_csv_and_plot(results_csv, network)"]},{"cell_type":"markdown","metadata":{"id":"GvoFRe5RjLqN"},"source":["# Test Transferibility"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nCO_4aa_jPX0"},"outputs":[],"source":["PERFORM_ATTACK_NN2 = False"]},{"cell_type":"markdown","metadata":{"id":"S_DSu2lnjVA8"},"source":["### Load list of images and labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WLtoVjTRjUbU"},"outputs":[],"source":["\n","data_transform = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor()\n","])\n","\n","if IN_COLAB:\n","  test_set_path = \"/content/drive/Shareddrives/AI4CYBSEC/face_dataset/test_set_MTCNN_NN2\"\n","else:\n","  test_set_path = \"./face_dataset/test_set_MTCNN_NN2\"\n","# Define dataset\n","dataset = VGGFace2Dataset(root_dir=test_set_path, image_size=(224,224), transform=data_transform)\n","\n","# Check the length of the dataset\n","print(\"Dataset length:\", len(dataset))\n","\n","batch_size=1\n","dataloader_NN2 = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n","\n","images_list = []\n","labels_list = []\n","images_adv_list = []\n","for image, label in dataloader_NN2:\n","    image_numpy = image.numpy()\n","    images_list.append(image_numpy)\n","    labels_list.append(label)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Qig4Ewpjlta"},"outputs":[],"source":["# dobbiamo fare la trasformazione che fa SENET\n","correct = 0\n","for  img, label in tqdm(zip(images_list, labels_list), total=len(images_list), desc=\" Validation on clean data with make_inference_NN2 RESNET LOADER\"):\n","    img = torch.tensor(img)\n","\n","    x_test_adv_pred = make_inference_NN2(model, img, device, with_preprocessing=True)\n","    if x_test_adv_pred == id_label_dict[label[0]]:\n","      correct +=1\n","accuracy_clean_data_NN2 = correct/len(images_list)\n","print(\"Accuracy on clean data: \", accuracy_clean_data_NN2)\n"]},{"cell_type":"markdown","metadata":{"id":"Rrp7GW2_jp9g"},"source":["### Set variable Perform Attack"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yd5BEIcFjvS0"},"outputs":[],"source":["if IN_COLAB:\n","    results_csv = \"/content/drive/Shareddrives/AI4CYBSEC/results/FGSM/attack_results_NN2.csv\"\n","else:\n","    results_csv = \"./results/FGSM/attack_results_NN2.csv\"\n","\n","network = \"NN2\""]},{"cell_type":"markdown","metadata":{"id":"U3WXOst6kHj1"},"source":["### Perform attack Error Generic Gray Box"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bR23WlUlRCpY"},"outputs":[],"source":["if PERFORM_ATTACK_NN2:\n","  from tqdm import tqdm\n","\n","  epsilon = 0.1\n","  targeted = False\n","  fgsm_untargeted_attacker = FastGradientMethod(estimator=classifier, eps=epsilon, targeted=targeted)\n","\n","  eps_range = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n","  accuracy_values_untargeted = [accuracy_clean_data_NN2]\n","  avg_perturbations = [0]\n","\n","  for eps in tqdm(eps_range, desc='Generating {} attacks'.format(attack_name)):\n","      fgsm_untargeted_attacker.set_params(**{'eps': eps})\n","      nb_correct_eps = 0\n","      images_adv_list = []\n","      for img, label in zip(images_list, labels_list):\n","          x_test_adv = fgsm_untargeted_attacker.generate(img)\n","          images_adv_list.append(x_test_adv)\n","          x_test_adv = torch.tensor(x_test_adv)\n","          x_test_adv_pred = make_inference_NN2(model, x_test_adv, device, with_preprocessing=True)\n","\n","          if x_test_adv_pred == id_label_dict[label[0]]:\n","            nb_correct_eps +=1\n","      accuracy_values_untargeted.append(nb_correct_eps/dataset_len)\n","      avg_perturbations.append(compute_perturbation(images_list, images_adv_list))\n","\n","  eps_range = [0] + eps_range\n","  constant_values = {}\n","  strength_name='eps'\n","  target_class = None\n","  accuracy_on_target_class = None\n","\n","  sec_curve(eps_range, accuracy_values_untargeted, constant_values, strength_name, target_class, attack_name, avg_perturbations, accuracy_on_target_class, network)\n","\n","  # save results on csv\n","  save_to_csv(attack_name, targeted, target_class, strength_name, eps_range, accuracy_values_untargeted, constant_values, avg_perturbations, results_csv)"]},{"cell_type":"markdown","metadata":{"id":"YLpVJ_7fRN1h"},"source":["### Perform attack Error Specific Gray Box"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"03YfSb27RUAl"},"outputs":[],"source":["if PERFORM_ATTACK_NN2:\n","  from tqdm import tqdm\n","\n","  epsilon = 0.1\n","  targeted=True\n","  fgsm_targeted_attacker = FastGradientMethod(estimator=classifier, eps=epsilon, targeted=targeted)\n","  target_class = 0\n","\n","  eps_range = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n","  accuracy_values_targeted = [accuracy_clean_data_NN2]\n","  accuracy_on_target_class = [0]\n","  avg_perturbations = [0]\n","\n","  targeted_label = np.array(target_class)\n","  one_hot_bob_targeted_label = np.expand_dims(tf.keras.utils.to_categorical(targeted_label, num_classes=nb_classes), axis=0)\n","\n","  for eps in tqdm(eps_range, desc='Generating {} attacks'.format(attack_name)):\n","      fgsm_targeted_attacker.set_params(**{'eps': eps})\n","      nb_correct_eps = 0\n","      nb_target_eps = 0\n","      nb_misclassifications = 0\n","      images_adv_list = []\n","      for img, label in zip(images_list, labels_list):\n","          x_test_adv = fgsm_targeted_attacker.generate(img, one_hot_bob_targeted_label)\n","          images_adv_list.append(x_test_adv)\n","          x_test_adv = torch.tensor(x_test_adv)\n","          x_test_adv_pred = make_inference_NN2(model, x_test_adv, device, with_preprocessing=True)\n","          if x_test_adv_pred == id_label_dict[label[0]]:\n","            nb_correct_eps +=1\n","          else:\n","            nb_misclassifications += 1\n","            if x_test_adv_pred == target_class:\n","              nb_target_eps += 1\n","\n","      accuracy_values_targeted.append(nb_correct_eps/dataset_len)\n","      accuracy_on_target_class.append(round(nb_target_eps/nb_misclassifications,3))\n","      avg_perturbations.append(compute_perturbation(images_list, images_adv_list))\n","\n","  eps_range = [0] + eps_range\n","  constant_values = {}\n","  strength_name='eps'\n","  sec_curve(eps_range, accuracy_values_targeted, constant_values, strength_name, target_class, attack_name, avg_perturbations, accuracy_on_target_class, network)\n","\n","  # save results on csv\n","  save_to_csv(attack_name, targeted, target_class, strength_name, eps_range, accuracy_values_targeted, constant_values, avg_perturbations, results_csv, accuracy_on_target_class)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"43iciiEjRzow"},"outputs":[],"source":["if IN_COLAB:\n","    results_csv = \"/content/drive/Shareddrives/AI4CYBSEC/results/FGSM/attack_results_NN2.csv\"\n","else:\n","    results_csv = \"./results/FGSM/attack_results_NN2.csv\"\n","\n","network = \"NN2\"\n","read_csv_and_plot(results_csv, network)"]},{"cell_type":"markdown","metadata":{"id":"Wd4iJHVbOK7B"},"source":["### FGSM NN1 with Defense"]},{"cell_type":"markdown","metadata":{"id":"Nc9_iccqOK7B"},"source":["#### Load Robus Detector for pre-processing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"urBxlPk_OK7B"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","if IN_COLAB:\n","    directory = \"/content/drive/Shareddrives/AI4CYBSEC/models\"\n","else:\n","    directory = \".\\models\"\n","def load_model(model, model_path):\n","    model.load_state_dict(torch.load(model_path))\n","    model.eval()\n","    return model\n","\n","# Definisci il modello mobilenet_v2\n","model = models.mobilenet_v2(pretrained=True)\n","\n","# Sostituisci il classificatore dell'ultimo layer con un nuovo classificatore\n","model.classifier[1] = nn.Linear(model.classifier[1].in_features, 2)\n","\n","model = model.to(device)\n","\n","\n","# carica i pesi del modello addestrato\n","defence = load_model(model, os.path.join(directory,'mobilenetv2_best_binary_classifier.pth'))\n","\n","import torchsummary\n","\n","# Stampa un riassunto del modello\n","torchsummary.summary(defence, (3, 160, 160))"]},{"cell_type":"markdown","metadata":{"id":"0SpZH4lwOK7C"},"source":["#### Utility for inferece"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j3e2ybrgOK7C"},"outputs":[],"source":["def make_inference_defence(model, img_tensor, device):\n","    #img must be a tensor with shape (N, C, H, W)\n","    model.eval()\n","    img_tensor = img_tensor.to(device)\n","    with torch.no_grad():\n","        outputs = model(img_tensor)\n","        _, predicted = torch.max(outputs, 1)\n","\n","    return predicted.item()\n","\n","def make_inference_NN1_with_defense(model,img_tensor, name_to_id, defense_model, device, isClean):\n","    model.to(device)\n","    defense_model.to(device)\n","    prediction_defense = make_inference_defence(defense_model, img_tensor, device)\n","    if prediction_defense == 1:\n","        if not isClean:\n","            return 1, None\n","        return 0 ,None\n","\n","    return make_inference(model, img_tensor, name_to_id, device)\n","\n","def validate_with_defence(dataloader, model, name_to_id, device, defence_model, clean_data = True):\n","    \"\"\"\n","    Validates a model on a dataset and returns the accuracy.\n","\n","    Args:\n","        dataset: Dataloader to validate the model on.\n","        model: Model to validate.\n","        device: Device to perform inference on.\n","\n","    Returns:\n","        accuracy: Accuracy of the model on the dataset.\n","    \"\"\"\n","    model.to(device)\n","    model.eval()\n","    correct_predictions = 0\n","    total_samples = len(dataloader) * dataloader.batch_size\n","    num_skipped_samples = 0\n","    with torch.no_grad():  # Disable gradient calculation\n","        for images, labels in tqdm(dataloader, desc=\"Validating model\"):\n","            predicted_classes, _= make_inference_NN1_with_defense(model, images, name_to_id, defence_model, device, clean_data)\n","            if predicted_classes == 1:# significa che ho predetto come adv un campione  adv\n","                num_skipped_samples += 1\n","                correct_predictions += 1\n","                continue\n","            elif predicted_classes == 0:# significa che ho predetto come adv un campione  clean\n","                num_skipped_samples += 1\n","                continue\n","            # DATO CHE C'è il continue è come se avessi un else:\n","            correct_predictions += sum(pred == label for pred, label in zip(predicted_classes, labels))\n","\n","\n","    # Compute accuracy\n","    accuracy = correct_predictions / total_samples\n","    return accuracy, num_skipped_samples"]},{"cell_type":"markdown","metadata":{"id":"7O2UeCEDOK7C"},"source":["#### Validation on clean data with defense"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ncV_0wzcOK7D"},"outputs":[],"source":["# Create transform for image resizing and normalization\n","\n","data_transform = transforms.Compose([\n","    transforms.Resize((160, 160)),\n","    transforms.ToTensor()\n","])\n","\n","if IN_COLAB:\n","  test_set_path = \"/content/drive/Shareddrives/AI4CYBSEC/face_dataset/test_set_MTCNN\"\n","else:\n","  test_set_path = \"./face_dataset/test_set_MTCNN\"\n","# Define dataset\n","dataset = VGGFace2Dataset(root_dir=test_set_path, transform=data_transform)\n","\n","# Check the length of the dataset\n","print(\"Dataset length:\", len(dataset))\n","\n","# Create DataLoader\n","batch_size = 1\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n","acc = validate(dataloader, resnet, name_to_id, device)\n","print(\"\\n Accuracy without defense \"+str(acc))\n","\n","acc_with_defence, num_skipped_samples = validate_with_defence(dataloader, resnet, name_to_id, device, defence, clean_data = True)\n","print(\"\\n Accuracy with defense \"+str(acc_with_defence))\n"]},{"cell_type":"markdown","metadata":{"id":"BjhTCndoOK7D"},"source":["### FGSM attack with defence -- NN1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WyZB_ft5OK7D"},"outputs":[],"source":["PERFORM_ATTACK_NN1_with_defense= False"]},{"cell_type":"markdown","metadata":{"id":"uznq05UPOK7D"},"source":["#### Load list of images and labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"odXIPyrHOK7E"},"outputs":[],"source":["images_list = []\n","labels_list = []\n","images_adv_list = []\n","for image, label in dataloader:\n","    # Effettua le predizioni del modello\n","    # image numpy on device\n","    image_numpy = image.numpy()\n","    images_list.append(image_numpy)\n","    labels_list.append(label)"]},{"cell_type":"markdown","metadata":{"id":"BpCzWIiKOK7E"},"source":["#### Set variable Perform Attack"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gqtVmkEXOK7E"},"outputs":[],"source":["if IN_COLAB:\n","    results_csv = \"/content/drive/Shareddrives/AI4CYBSEC/results/FGSM/attack_results_NN1_with_defense.csv\"\n","else:\n","    results_csv = \"./results/FGSM/attack_results_NN1_with_defense.csv\"\n","network = \"NN1_with_defense\"\n","read_csv_and_plot(results_csv, network)"]},{"cell_type":"markdown","metadata":{"id":"39MeNnOzOK7E"},"source":["#### Perform attack Error Generic Gray Box"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y7FD1NxoOK7E"},"outputs":[],"source":["if PERFORM_ATTACK_NN1_with_defense:\n","  from tqdm import tqdm\n","  from art.attacks.evasion import FastGradientMethod\n","  import time\n","\n","  from tqdm import tqdm\n","\n","  epsilon = 0.1\n","  targeted = False\n","  fgsm_untargeted_attacker = FastGradientMethod(estimator=classifier, eps=epsilon, targeted=targeted)\n","\n","  eps_range = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n","  accuracy_values_untargeted = [acc_with_defence]\n","  avg_perturbations = [0]\n","\n","  for eps in tqdm(eps_range, desc='Generating {} attacks'.format(attack_name)):\n","      fgsm_untargeted_attacker.set_params(**{'eps': eps})\n","      num_skipped_samples = 0\n","      nb_correct_eps = 0\n","      images_adv_list = []\n","      for img, label in zip(images_list, labels_list):\n","        x_test_adv = fgsm_untargeted_attacker.generate(img)\n","        images_adv_list.append(x_test_adv)\n","        x_test_adv = torch.tensor(x_test_adv)\n","        x_test_adv_pred, _= make_inference_NN1_with_defense(resnet, x_test_adv, name_to_id, defence, device, isClean=False)\n","\n","        if x_test_adv_pred == 1:# significa che ho predetto come adv un campione  adv\n","          num_skipped_samples += 1\n","          nb_correct_eps += 1\n","          continue\n","        elif x_test_adv_pred == 0:# significa che ho predetto come adv un campione  clean\n","            num_skipped_samples += 1\n","            continue\n","        if x_test_adv_pred[0] == label[0]:\n","          nb_correct_eps +=1\n","      accuracy_values_untargeted.append(nb_correct_eps/len(images_list))\n","      avg_perturbations.append(compute_perturbation(images_list, images_adv_list))\n","      print(\"accuracy: \", nb_correct_eps/len(images_list))\n","      print(\"Number of skipped samples: \", num_skipped_samples)\n","\n","  eps_range = [0] + eps_range\n","  constant_values = {}\n","  strength_name='eps'\n","  target_class = None\n","  accuracy_on_target_class = None\n","\n","  sec_curve(eps_range, accuracy_values_untargeted, constant_values, strength_name, target_class, attack_name, avg_perturbations, accuracy_on_target_class, network)\n","\n","  # save results on csv\n","  save_to_csv(attack_name, targeted, target_class, strength_name, eps_range, accuracy_values_untargeted, constant_values, avg_perturbations, results_csv)"]},{"cell_type":"markdown","metadata":{"id":"XefPuMIAOK7F"},"source":["#### Perform attack Error Specific Gray Box"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fPBo9MNkOK7F"},"outputs":[],"source":["if PERFORM_ATTACK_NN1_with_defense:\n","  from tqdm import tqdm\n","  from art.attacks.evasion import FastGradientMethod\n","  import time\n","\n","  from tqdm import tqdm\n","\n","  epsilon = 0.1\n","  targeted=True\n","  fgsm_targeted_attacker = FastGradientMethod(estimator=classifier, eps=epsilon, targeted=targeted)\n","  target_class = 0\n","\n","  eps_range = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n","  accuracy_values_untargeted = [acc_with_defence]\n","  avg_perturbations = [0]\n","\n","  targeted_label = np.array(target_class)\n","  one_hot_bob_targeted_label = np.expand_dims(tf.keras.utils.to_categorical(targeted_label, num_classes=nb_classes), axis=0)\n","\n","  for eps in tqdm(eps_range, desc=f'Generating {attack_name}'):\n","      fgsm_targeted_attacker.set_params(**{'eps': eps, 'eps_step':eps/10})\n","      num_skipped_samples = 0\n","      nb_correct_eps = 0\n","      error_equl_target=0\n","      images_adv_list = []\n","      for img, label in zip(images_list, labels_list):\n","          x_test_adv = fgsm_targeted_attacker.generate(img, one_hot_bob_targeted_label)\n","          images_adv_list.append(x_test_adv)\n","          x_test_adv = torch.tensor(x_test_adv)\n","          x_test_adv_pred, x_test_adv_pred_model = make_inference_NN1_with_defense(resnet, x_test_adv, name_to_id, defence, device, isClean=False)\n","          if x_test_adv_pred == 1:# significa che ho predetto come adv un campione  adv\n","            num_skipped_samples += 1\n","            nb_correct_eps += 1\n","            continue\n","          elif x_test_adv_pred == 0:# significa che ho predetto come adv un campione clean\n","              num_skipped_samples += 1\n","              continue\n","          if x_test_adv_pred[0] == label[0]:\n","            nb_correct_eps +=1\n","          elif x_test_adv_pred_model[0] == target_class:\n","            error_equl_target+=1\n","      accuracy_values_targeted.append(round(nb_correct_eps/dataset_len,3))\n","      avg_perturbations.append(compute_perturbation(images_list, images_adv_list))\n","\n","      if len(images_list)-nb_correct_eps == 0:\n","          res = 0\n","      else:\n","          res = round(error_equl_target/(len(images_list)-nb_correct_eps),3)\n","      accuracy_on_target_class.append(res)\n","      print(\"accuracy: \", nb_correct_eps/len(images_list))\n","      print(f\"avg_perturbations: {avg_perturbations}\")\n","      print(\"Number of skipped samples: \", num_skipped_samples)\n","      print(f\"accuracy_on_target_class: {accuracy_on_target_class}\")\n","\n","  eps_range = [0] + eps_range\n","  constant_values = {}\n","  strength_name='eps'\n","  sec_curve(eps_range, accuracy_values_targeted, constant_values, strength_name, target_class, attack_name, avg_perturbations, accuracy_on_target_class, network)\n","\n","  # save results on csv\n","  save_to_csv(attack_name, targeted, target_class, strength_name, eps_range, accuracy_values_targeted, constant_values, avg_perturbations, results_csv, accuracy_on_target_class)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tZkD3Q68OK7F"},"outputs":[],"source":["if IN_COLAB:\n","    results_csv = \"/content/drive/Shareddrives/AI4CYBSEC/results/FGSM/attack_results_NN1_with_defense.csv\"\n","else:\n","    results_csv = \"./results/FGSM/attack_results_NN1_with_defense.csv\"\n","network = \"NN1_with_defense\"\n","read_csv_and_plot(results_csv, network)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}