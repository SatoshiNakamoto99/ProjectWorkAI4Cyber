{"cells":[{"cell_type":"markdown","metadata":{"id":"YbJM10vaR1Br"},"source":["## 1. Loading prereqs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["try:\n","    from google.colab import drive\n","    IN_COLAB = True\n","    print(\"Running on Google Colab. \")\n","except:\n","    IN_COLAB = False\n","    print(\"Not running on Google Colab. \")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9884,"status":"ok","timestamp":1715371958374,"user":{"displayName":"ANTONIO NOCERINO","userId":"15400768308026658847"},"user_tz":-120},"id":"HQ-NGoOGNt2I","outputId":"35cb9727-fba5-4bab-e372-76b328046e48"},"outputs":[],"source":["if IN_COLAB:\n","    !pip install facenet-pytorch  # fornisce modelli pre-addestrati PyTorch per compiti di riconoscimento facciale\n","    !pip install Pillow # aggiunge il supporto per l'apertura, la manipolazione e il salvataggio di molti diversi formati di file immagine."]},{"cell_type":"markdown","metadata":{"id":"hSU0qGu7Z5cr"},"source":["## 2. Load NN1"]},{"cell_type":"markdown","metadata":{"id":"f1ycl1LPTqsz"},"source":["#### Load pre-trained model\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["eb02ac66179e437b8d5b9787b524e2da","af98156f838648ad9989fbefd7168ae2","4605223a2d3349f7b82aac4c4a6a82e4","a1528190d8eb4df2993af5233bd77e05","84e94d9a4de849d1a311adb65a50227a","9afb7e2f1f8546909d642c476cd65135","ee43aab718474ef9b1c82acf5a98ddb8","ba3cc69dc77549328c7b637e9edbe1e3","7f62197a0e6a40d6ac86ab69a1d9fdbd","26dcbc5e6c184258b05207151b8f0e3d","2bf05f045d3b47abb7b2f0f33cc5ea0e"]},"executionInfo":{"elapsed":11310,"status":"ok","timestamp":1715371969679,"user":{"displayName":"ANTONIO NOCERINO","userId":"15400768308026658847"},"user_tz":-120},"id":"IXc7QMPuSeco","outputId":"4f11c005-f96c-465f-f1bc-efe4491e83d0"},"outputs":[],"source":["# utilizzo la libreria facenet_pytorch per caricare il modello InceptionResnetV1 preaddestrato sul dataset VGGFace2 e abilitare la classificazione.\n","from facenet_pytorch import InceptionResnetV1, MTCNN\n","import torch\n","# image_size = 160\n","# margin = 0\n","# mtcnn = MTCNN(image_size,margin)\n","\n","resnet = InceptionResnetV1(pretrained='vggface2').eval()\n","resnet.classify = True\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print('Running on device: {}'.format(device))\n","resnet = resnet.to(device)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GzhAQvqdS9Ko"},"source":["#### Loading labels of model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8054,"status":"ok","timestamp":1715371980897,"user":{"displayName":"ANTONIO NOCERINO","userId":"15400768308026658847"},"user_tz":-120},"id":"Tc_BifakDUrs","outputId":"fe7babd6-2462-472f-a887-c0553221ad35"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","\n","# Il modello è addestrato sulle seguenti Labels:\n","# Carico le labels del dataset VGGFACE\n","fpath = tf.keras.utils.get_file('rcmalli_vggface_labels_v2.npy',\n","                             \"https://github.com/rcmalli/keras-vggface/releases/download/v2.0/rcmalli_vggface_labels_v2.npy\",\n","                             cache_subdir=\"./\")\n","LABELS = np.load(fpath) # List of name\n","# Clean list of name\n","#name_LABELS={}\n","for i in range(len(LABELS)):\n","  LABELS[i] = LABELS[i].strip().replace(' ', '').replace('\"', '')\n","  #name_LABELS[LABELS[i]]=i"]},{"cell_type":"markdown","metadata":{"id":"Cepg-ZkuZgbH"},"source":["## 3. Load Test Set"]},{"cell_type":"markdown","metadata":{"id":"N-4qw1-MTw2m"},"source":["#### Connect to Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19247,"status":"ok","timestamp":1715372001873,"user":{"displayName":"ANTONIO NOCERINO","userId":"15400768308026658847"},"user_tz":-120},"id":"-P6LcqwHEkk8","outputId":"07b18443-c19e-4ecb-a7a0-fa8e8f370229"},"outputs":[],"source":["if IN_COLAB:\n","    from google.colab import drive\n","    drive.mount('/content/drive/')"]},{"cell_type":"markdown","metadata":{"id":"L-NwB2gKZDLX"},"source":["#### Load the label of Test Set (for NN1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CaAhwqLCXx4Q"},"outputs":[],"source":["# set the path for the dataset\n","\n","if IN_COLAB:\n","    path_dataset = \"/content/drive/Shareddrives/AI4CYBSEC/face_dataset\"\n","else:\n","    path_dataset = \"./face_dataset\"\n","identity_meta_NN1_name = \"meta_identity_NN1.csv\"\n","\n","import pandas as pd\n","import os\n","\n","path_identity_csv =os.path.join(path_dataset,identity_meta_NN1_name)\n","identity_meta_NN1 = pd.read_csv(path_identity_csv)\n"]},{"cell_type":"markdown","metadata":{"id":"2H276KCNaKsJ"},"source":["## 4. Mapping different label encoding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"evxVsnB6aPTG"},"outputs":[],"source":["# I want a dictonary related to the label of the Test Set that map the name of celebrities with label associated\n","name_to_id = {}\n","id_to_name = {}\n","for index, row in identity_meta_NN1.iterrows():\n","    # Ora puoi accedere ai valori di ogni riga come segue:\n","    class_id = row['Class_ID']\n","    name = row['Name']\n","    name_to_id[name]=class_id\n","    id_to_name[class_id] = name\n","\n"]},{"cell_type":"markdown","metadata":{"id":"o5aQtJxlwUE4"},"source":["## 5. Dataset Class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YCGxCH5YqSQQ"},"outputs":[],"source":["from PIL import Image\n","import os\n","import torch\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader\n","\n","class VGGFace2Dataset(Dataset):\n","    def __init__(self, root_dir, image_size=(160, 160), transform=None):\n","        self.root_dir = root_dir\n","        self.image_size = image_size\n","        self.transform = transform\n","\n","        # List of files in the dataset\n","        self.file_list = []\n","        for root, dirs, files in os.walk(self.root_dir):\n","            for file in files:\n","                self.file_list.append(os.path.join(root, file))\n","\n","    def __len__(self):\n","        return len(self.file_list)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.file_list[idx]\n","        img = Image.open(img_path).resize(self.image_size)\n","\n","        # Extract the label from the file path\n","        label = os.path.split(os.path.dirname(img_path))[-1]\n","\n","        if self.transform:\n","            img = self.transform(img)\n","\n","        return img, label\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"m33yvIJoSUkr"},"source":["## 6. Evaluate model NN1\n"]},{"cell_type":"markdown","metadata":{"id":"foZQE3LkdGCj"},"source":["### Utility Function for NN1 with mapping labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5gjQcj8Ij6gr"},"outputs":[],"source":["\n","from PIL import Image\n","from torchvision import transforms\n","import torch\n","import numpy as np\n","from matplotlib import pyplot as plt\n","from tqdm import tqdm\n","\n","def load_image(file_path):\n","    \"\"\" carica un'immagine da un percorso e la apre come un'immagine utilizzando Image.open dal modulo Pillow.\n","    Successivamente, ridimensiona l'immagine a dimensioni 160x160 pixel e la converte in un tensore utilizzando\n","    transforms.ToTensor() dal modulo torchvision.transforms.\n","    Infine, restituisce sia il tensore dell'immagine che l'immagine aperta.\n","    \"\"\"\n","    rsz = Image.open(file_path).resize((160, 160))\n","    tns = transforms.ToTensor()(rsz)\n","    return tns, rsz\n","def make_inference(model, image_tensors, name_to_id, device):\n","    \"\"\"\n","    Takes input image tensor and returns the label associated with the network's prediction.\n","    \"\"\"\n","    # Move image tensors to the specified device\n","    image_tensors = image_tensors.to(device)\n","\n","    probs = model(image_tensors)\n","    #print(\"probs\", probs)\n","\n","    # Get the number of elements along the first dimension\n","    num_elements = probs.size(0)\n","\n","    # Initialize two lists to store the argmax\n","    argmax_list_1 = []\n","    argmax_list_2 = []\n","\n","    # Compute argmax for each element along the first dimension\n","    for i in range(num_elements):\n","        target_class = np.array(probs[i].detach().cpu().numpy()).argmax()  # Move to CPU for numpy operations\n","        argmax_list_1.append(name_to_id[LABELS[target_class]])\n","        argmax_list_2.append(target_class)\n","\n","    return argmax_list_1, argmax_list_2\n","\n","def validate(dataset, model, name_to_id, device):\n","    \"\"\"\n","    Validates a model on a dataset and returns the accuracy.\n","\n","    Args:\n","        dataset: Dataloader to validate the model on.\n","        model: Model to validate.\n","        device: Device to perform inference on.\n","\n","    Returns:\n","        accuracy: Accuracy of the model on the dataset.\n","    \"\"\"\n","    model.eval()\n","    correct_predictions = 0\n","    total_samples = len(dataset) * dataset.batch_size\n","\n","    with torch.no_grad():  # Disable gradient calculation\n","        for images, labels in tqdm(dataset, desc=\"Validating model\"):\n","            #images = mtcnn(images)\n","            predicted_classes, _ = make_inference(model, images, name_to_id, device)\n","            correct_predictions += sum(pred == label for pred, label in zip(predicted_classes, labels))\n","\n","    # Compute accuracy\n","    accuracy = correct_predictions / total_samples\n","    return accuracy\n","\n","\n","def plot_image(original_image, original_label):\n","  \"\"\"\n","  prende in ingresso le PIL.Image del campione originale e del corrispondete adversarial sample e li plotta\n","  \"\"\"\n","  plt.figure()\n","  plt.matshow(original_image)\n","  plt.title(\"Model Prediction: {}\".format(original_label))\n","  plt.show()"]},{"cell_type":"markdown","metadata":{"id":"1e3M3Ojj2h4R"},"source":["### Validation on Clean Data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":598272,"status":"ok","timestamp":1715372613199,"user":{"displayName":"ANTONIO NOCERINO","userId":"15400768308026658847"},"user_tz":-120},"id":"KZF6hf0V2gZO","outputId":"09150c54-0fe1-4f22-fd11-ec1862f45ff1"},"outputs":[],"source":["# Create transform for image resizing and normalization\n","data_transform = transforms.Compose([\n","    transforms.Resize((160, 160)),\n","    transforms.ToTensor()\n","])\n","\n","if IN_COLAB:\n","    test_set_path = \"/content/drive/Shareddrives/AI4CYBSEC/face_dataset/test_set_MTCNN\"\n","else:\n","    test_set_path = \"./face_dataset/test_set_MTCNN\"\n","# Define dataset\n","dataset = VGGFace2Dataset(root_dir=test_set_path, transform=data_transform)\n","\n","# Check the length of the dataset\n","print(\"Dataset length:\", len(dataset))\n","\n","# Create DataLoader\n","batch_size = 1\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n","\n","acc = validate(dataloader, resnet, name_to_id, device)\n","print(\"\\n\"+str(acc))\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"26dcbc5e6c184258b05207151b8f0e3d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2bf05f045d3b47abb7b2f0f33cc5ea0e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4605223a2d3349f7b82aac4c4a6a82e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba3cc69dc77549328c7b637e9edbe1e3","max":111898327,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7f62197a0e6a40d6ac86ab69a1d9fdbd","value":111898327}},"7f62197a0e6a40d6ac86ab69a1d9fdbd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"84e94d9a4de849d1a311adb65a50227a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9afb7e2f1f8546909d642c476cd65135":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1528190d8eb4df2993af5233bd77e05":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_26dcbc5e6c184258b05207151b8f0e3d","placeholder":"​","style":"IPY_MODEL_2bf05f045d3b47abb7b2f0f33cc5ea0e","value":" 107M/107M [00:00&lt;00:00, 140MB/s]"}},"af98156f838648ad9989fbefd7168ae2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9afb7e2f1f8546909d642c476cd65135","placeholder":"​","style":"IPY_MODEL_ee43aab718474ef9b1c82acf5a98ddb8","value":"100%"}},"ba3cc69dc77549328c7b637e9edbe1e3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb02ac66179e437b8d5b9787b524e2da":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_af98156f838648ad9989fbefd7168ae2","IPY_MODEL_4605223a2d3349f7b82aac4c4a6a82e4","IPY_MODEL_a1528190d8eb4df2993af5233bd77e05"],"layout":"IPY_MODEL_84e94d9a4de849d1a311adb65a50227a"}},"ee43aab718474ef9b1c82acf5a98ddb8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
